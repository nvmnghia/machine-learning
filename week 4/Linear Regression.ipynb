{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hồi quy tuyến tính\n",
    "*Hoàn thành toàn bộ phần bài tập trong notebook này, bao gồm toàn bộ kết quả đầu ra và code hỗ trợ.*\n",
    "\n",
    "***\"Không có một sự kiện nào trên đời là ngẫu nhiên, những thứ đang cho là ngẫu nhiên chỉ là những sự kiện ta chưa tìm ra được mô hình để biểu diễn quy luật của chúng\".***\n",
    "\n",
    "Xây dựng mô hình **Hồi quy tuyến tính** bao gồm hai phần:\n",
    "- Trong quá trình huấn luyện, bộ phân lớp lấy dữ liệu huấn luyện và và học các tham số mô hình.\n",
    "- Trong quá trình kiếm tra, mô hình phân lớp từng đối tượng bằng cách nhân giá trị của mẫu với các tham số mô hình để tìm ra giá trị của nhãn.\n",
    "- Giá trị của tham số được kiểm định chéo.\n",
    "Trong bài tập này, bạn sẽ cài đặt những bước trên và hiểu được qui trình Xây dựng một mô hình đơn giản với Học tham số, kiểm định chéo, và hiểu được cách viết code hiệu quả với vectorize.\n",
    "\n",
    "Bài toán dự đoán giá nhà Boston được sử dụng trong bài tập này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import một số thư viện cần thiết.\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sử dụng một mẹo nhỏ để vẽ hình trên cùng một dòng thay vì mở cửa sổ mới\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # đặt kích thước mặc định cho hình\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Một mẹo nhỏ để notebook tự load lại các module bên ngoài;\n",
    "# xem thêm tại http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (404, 13)\n",
      "Training labels shape:  (404,)\n",
      "Test data shape:  (102, 13)\n",
      "Test labels shape:  (102,)\n"
     ]
    }
   ],
   "source": [
    "# Tải dữ liệu Giá nhà Boston từ Scikit-learn.\n",
    "boston = datasets.load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, \\\n",
    "                                                    boston.target, test_size=0.2)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dữ liệu\n",
    "Hồi qui tuyến tính đơn giản là một cách tiếp cận để dự đoán phản ứng (giá trị đầu ra) khi dữ liệu có một đặc trưng duy nhất. Khi giả sử hai biến $x$ và $y$ liên hệ tuyến  tính thì mục tiêu của mô hình là cố tìm ra đường tuyến tính tốt nhất để dự đoán phản ứng ($y$). \n",
    "\n",
    "Đường đó được gọi là đường hồi quy.\n",
    "\n",
    "Công thức cho đường hồi quy được biểu diễn như sau:\n",
    "$$ \\hat{Y} = h(X) = XW$$\n",
    "Trong đó: \n",
    "\n",
    "- $X$ là ma trận có kích thước $N \\times D$ với $X_{ij}$ là giá trị của đặc trưng thứ $j$ của mẫu $i$.\n",
    "- $W$ là ma trận tham số có kích thước $D \\times 1$\n",
    "- $Y$ là giá trị phản ứng của $N$ mẫu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22f1a466048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Biểu diễn một số ví dụ trong tập huấn luyện sử dụng một đặc trưng duy nhất.\n",
    "# LSTAT - % lower status of the population\n",
    "plt.scatter(X_train[:,12], y_train)\n",
    "plt.xlabel(\"Crime rate\")\n",
    "plt.ylabel(\"House's price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huấn luyện mô hình\n",
    "Tất cả code cho phần bài tập này được lưu trong tệp **models/linear_regression.py** và **models/linear_loss.py**.\n",
    "### Cập nhật tham số\n",
    "Quá trình huấn luyện mô hình thực chất là từ dữ liệu để học ra tham số mô hình phù hợp nhất với mô hình sinh dữ liệu. Trong mô hình hồi quy tuyến tính, ta cần học tham số $W$.\n",
    "\n",
    "Khi khởi tạo mô hình, ta giả sử tham số được khởi tạo ngẫu nhiên. Sử dụng tham số $W$ đó, ta ước lượng được giá trị $Y$:\n",
    "$$ \\hat{y} = h(X) = WX $$\n",
    "\n",
    "Tổng sai số, độ lệch của giá trị dự đoán so với giá trị thực tế gọi là hàm giá trị (Cost function):\n",
    "$$ J(w) = \\frac{1}{2N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2 = \\frac{1}{2N}\\sum_{i=1}^{N} (w_ix_i - y_i)^2$$\n",
    "\n",
    "\n",
    "Chúng ta sử thuật toán **xuống đồi (Gradient descent)** để tối ưu tham số $W$. (Xem khóa [Machine Learning](https://www.coursera.org/learn/machine-learning/))\n",
    "\n",
    "Đột tụt dốc của tham số $W$ được cập nhật theo công thức:\n",
    "$$ dw_i = \\frac{\\partial}{\\partial w_i}J(w)$$\n",
    "\n",
    "Đầu tiên, mở file ```models/linear_loss.py``` và cài đặt hàm ```linear_loss_naive```, sử dụng vòng lặp để tính hàm giá trị (Cost function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 294.769921\n"
     ]
    }
   ],
   "source": [
    "from models.linear_loss import linear_loss_naive\n",
    "import time\n",
    "\n",
    "# sinh ngẫu nhiên các trọng số (W) với các giá trị nhỏ\n",
    "W = np.random.randn(13, ) * 0.0001 \n",
    "\n",
    "loss, grad = linear_loss_naive(W, X_test, y_test, 0.00001)\n",
    "print('loss: %f' % (loss, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lúc này, các giá trị gradient được trả về đều bằng 0. Đạo hàm và tính gradient theo công thức được cho ở trên trong cùng hàm ```linear_loss_naive```. Bạn sẽ thấy một số thứ hữu ích trong phần cài đặt trước đó.\n",
    "\n",
    "Để đảm bảo là bạn đã cài đặt đúng, chúng ta sẽ sử dụng hàm ```grad_check_sparse``` (đã được cài đặt sẵn) để kiểm tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -241.043871 analytic: -241.043871, relative error: 1.676935e-11\n",
      "numerical: -84.809857 analytic: -84.809857, relative error: 6.763237e-11\n",
      "numerical: -57.589207 analytic: -57.589207, relative error: 3.216197e-11\n",
      "numerical: -1523.338934 analytic: -1523.338934, relative error: 2.343383e-14\n",
      "numerical: -409.141037 analytic: -409.141037, relative error: 6.247842e-13\n",
      "numerical: -215.900423 analytic: -215.900423, relative error: 6.519209e-12\n",
      "numerical: -3.510328 analytic: -3.510328, relative error: 3.218826e-10\n",
      "numerical: -57.589207 analytic: -57.589207, relative error: 3.216197e-11\n",
      "numerical: -3.510328 analytic: -3.510328, relative error: 3.218826e-10\n",
      "numerical: -274.636156 analytic: -274.636156, relative error: 1.815811e-12\n",
      "numerical: -409.141026 analytic: -409.141026, relative error: 1.492633e-12\n",
      "numerical: -146.175581 analytic: -146.175581, relative error: 2.564798e-11\n",
      "numerical: -244.348439 analytic: -244.348439, relative error: 1.857635e-11\n",
      "numerical: -8911.953748 analytic: -8911.953748, relative error: 2.970773e-13\n",
      "numerical: -84.809879 analytic: -84.809879, relative error: 6.392129e-11\n",
      "numerical: -57.589333 analytic: -57.589333, relative error: 5.876956e-11\n",
      "numerical: -8309.937721 analytic: -8309.937721, relative error: 8.493059e-14\n",
      "numerical: -409.141026 analytic: -409.141026, relative error: 1.492633e-12\n",
      "numerical: -215.900693 analytic: -215.900693, relative error: 1.874655e-12\n",
      "numerical: -146.175581 analytic: -146.175581, relative error: 2.564798e-11\n"
     ]
    }
   ],
   "source": [
    "# Bởi vì bạn đã cài đặt hàm gradient, tính toán gradient với code dưới đây và\n",
    "# kiểm tra với hàm grad_check_sparse(...) đã cho.\n",
    "\n",
    "# Tính toán loss và grad với W.\n",
    "loss, grad = linear_loss_naive(W, X_test, y_test, 0.0)\n",
    "\n",
    "# Tính toán gradient theo một số chiều ngẫu nhiên và so sánh chúng với kết quả\n",
    "# của bạn. Giá trị phải gần như chính xác theo tất cả các chiều.\n",
    "from models.gradient_check import grad_check_sparse\n",
    "f = lambda w: linear_loss_naive(w, X_test, y_test, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# thực hiện kiểm tra khi có sử dụng regularization\n",
    "# đừng quên cài đặt gradient với regularization nhé.\n",
    "loss, grad = linear_loss_naive(W, X_test, y_test, 1e2)\n",
    "f = lambda w: linear_loss_naive(w, X_test, y_test, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# Kết quả relative error trong khoảng 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 2.947699e+02 computed in 0.001502s\n",
      "Vectorized loss: 2.947699e+02 computed in 0.005484s\n",
      "difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Kế tiếp, cài đặt linear_loss_vectorized; hiện tại chỉ tính toán hàm giá trị;\n",
    "# gradient sẽ cài đặt sau.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = linear_loss_naive(W, X_test, y_test, 0.00001)\n",
    "toc = time.time()\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "# Vectorized\n",
    "from models.linear_loss import linear_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, _ = linear_loss_vectorized(W, X_test, y_test, 0.00001)\n",
    "toc = time.time()\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# Hàm giá trị khi vectorized nên có cùng giá trị với giá trị được tính bằng hàm\n",
    "# linear_loss_naive() nhưng tính toán nhanh hơn\n",
    "print('difference: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss and gradient: computed in 0.002001s\n",
      "Vectorized loss and gradient: computed in 0.000000s\n",
      "difference: 2.606103768001348e-13\n"
     ]
    }
   ],
   "source": [
    "# Hoàn thiện phần cài đặt của linear_loss_vectorized, và tính toán gradient theo\n",
    "# cách vectorized.\n",
    "\n",
    "# Hai hàm tính loss và gradient nên cho kết quả giống nhau nhưng bản vectorized \n",
    "# tính toán nhanh hơn.\n",
    "tic = time.time()\n",
    "_, grad_naive = linear_loss_naive(W, X_test, y_test, 0.00001)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = linear_loss_vectorized(W, X_test, y_test, 0.00001)\n",
    "toc = time.time()\n",
    "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "# So sánh gradient\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized)\n",
    "print('difference: {}'.format(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Huấn luyện với hàm cập nhật\n",
    "Sử dụng các hàm ```loss``` đã cài đặt ở trên để cài đặt hàm ```train``` trong tệp **linear_regression.py**.\n",
    "\n",
    "Tham số W được cập nhật từng thành phần theo công thức:\n",
    "$$ w_i =  w_i -\\alpha\\frac{\\partial}{\\partial w_i}J(w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 150000: loss 287.487854\n",
      "iteration 100 / 150000: loss 60.392558\n",
      "iteration 200 / 150000: loss 53.259228\n",
      "iteration 300 / 150000: loss 49.003306\n",
      "iteration 400 / 150000: loss 46.310625\n",
      "iteration 500 / 150000: loss 44.600179\n",
      "iteration 600 / 150000: loss 43.507267\n",
      "iteration 700 / 150000: loss 42.802686\n",
      "iteration 800 / 150000: loss 42.342385\n",
      "iteration 900 / 150000: loss 42.035831\n",
      "iteration 1000 / 150000: loss 41.826124\n",
      "iteration 1100 / 150000: loss 41.677498\n",
      "iteration 1200 / 150000: loss 41.567475\n",
      "iteration 1300 / 150000: loss 41.481924\n",
      "iteration 1400 / 150000: loss 41.411960\n",
      "iteration 1500 / 150000: loss 41.351997\n",
      "iteration 1600 / 150000: loss 41.298521\n",
      "iteration 1700 / 150000: loss 41.249320\n",
      "iteration 1800 / 150000: loss 41.203001\n",
      "iteration 1900 / 150000: loss 41.158686\n",
      "iteration 2000 / 150000: loss 41.115819\n",
      "iteration 2100 / 150000: loss 41.074048\n",
      "iteration 2200 / 150000: loss 41.033149\n",
      "iteration 2300 / 150000: loss 40.992980\n",
      "iteration 2400 / 150000: loss 40.953446\n",
      "iteration 2500 / 150000: loss 40.914487\n",
      "iteration 2600 / 150000: loss 40.876061\n",
      "iteration 2700 / 150000: loss 40.838140\n",
      "iteration 2800 / 150000: loss 40.800704\n",
      "iteration 2900 / 150000: loss 40.763736\n",
      "iteration 3000 / 150000: loss 40.727225\n",
      "iteration 3100 / 150000: loss 40.691160\n",
      "iteration 3200 / 150000: loss 40.655533\n",
      "iteration 3300 / 150000: loss 40.620335\n",
      "iteration 3400 / 150000: loss 40.585560\n",
      "iteration 3500 / 150000: loss 40.551201\n",
      "iteration 3600 / 150000: loss 40.517250\n",
      "iteration 3700 / 150000: loss 40.483702\n",
      "iteration 3800 / 150000: loss 40.450550\n",
      "iteration 3900 / 150000: loss 40.417788\n",
      "iteration 4000 / 150000: loss 40.385410\n",
      "iteration 4100 / 150000: loss 40.353411\n",
      "iteration 4200 / 150000: loss 40.321785\n",
      "iteration 4300 / 150000: loss 40.290526\n",
      "iteration 4400 / 150000: loss 40.259629\n",
      "iteration 4500 / 150000: loss 40.229088\n",
      "iteration 4600 / 150000: loss 40.198898\n",
      "iteration 4700 / 150000: loss 40.169054\n",
      "iteration 4800 / 150000: loss 40.139550\n",
      "iteration 4900 / 150000: loss 40.110382\n",
      "iteration 5000 / 150000: loss 40.081545\n",
      "iteration 5100 / 150000: loss 40.053034\n",
      "iteration 5200 / 150000: loss 40.024844\n",
      "iteration 5300 / 150000: loss 39.996970\n",
      "iteration 5400 / 150000: loss 39.969408\n",
      "iteration 5500 / 150000: loss 39.942153\n",
      "iteration 5600 / 150000: loss 39.915200\n",
      "iteration 5700 / 150000: loss 39.888547\n",
      "iteration 5800 / 150000: loss 39.862187\n",
      "iteration 5900 / 150000: loss 39.836117\n",
      "iteration 6000 / 150000: loss 39.810332\n",
      "iteration 6100 / 150000: loss 39.784829\n",
      "iteration 6200 / 150000: loss 39.759603\n",
      "iteration 6300 / 150000: loss 39.734651\n",
      "iteration 6400 / 150000: loss 39.709969\n",
      "iteration 6500 / 150000: loss 39.685552\n",
      "iteration 6600 / 150000: loss 39.661397\n",
      "iteration 6700 / 150000: loss 39.637500\n",
      "iteration 6800 / 150000: loss 39.613858\n",
      "iteration 6900 / 150000: loss 39.590466\n",
      "iteration 7000 / 150000: loss 39.567323\n",
      "iteration 7100 / 150000: loss 39.544423\n",
      "iteration 7200 / 150000: loss 39.521763\n",
      "iteration 7300 / 150000: loss 39.499341\n",
      "iteration 7400 / 150000: loss 39.477153\n",
      "iteration 7500 / 150000: loss 39.455195\n",
      "iteration 7600 / 150000: loss 39.433465\n",
      "iteration 7700 / 150000: loss 39.411959\n",
      "iteration 7800 / 150000: loss 39.390674\n",
      "iteration 7900 / 150000: loss 39.369607\n",
      "iteration 8000 / 150000: loss 39.348755\n",
      "iteration 8100 / 150000: loss 39.328116\n",
      "iteration 8200 / 150000: loss 39.307685\n",
      "iteration 8300 / 150000: loss 39.287461\n",
      "iteration 8400 / 150000: loss 39.267441\n",
      "iteration 8500 / 150000: loss 39.247621\n",
      "iteration 8600 / 150000: loss 39.228000\n",
      "iteration 8700 / 150000: loss 39.208574\n",
      "iteration 8800 / 150000: loss 39.189341\n",
      "iteration 8900 / 150000: loss 39.170297\n",
      "iteration 9000 / 150000: loss 39.151442\n",
      "iteration 9100 / 150000: loss 39.132772\n",
      "iteration 9200 / 150000: loss 39.114284\n",
      "iteration 9300 / 150000: loss 39.095977\n",
      "iteration 9400 / 150000: loss 39.077847\n",
      "iteration 9500 / 150000: loss 39.059893\n",
      "iteration 9600 / 150000: loss 39.042112\n",
      "iteration 9700 / 150000: loss 39.024502\n",
      "iteration 9800 / 150000: loss 39.007060\n",
      "iteration 9900 / 150000: loss 38.989785\n",
      "iteration 10000 / 150000: loss 38.972675\n",
      "iteration 10100 / 150000: loss 38.955726\n",
      "iteration 10200 / 150000: loss 38.938937\n",
      "iteration 10300 / 150000: loss 38.922307\n",
      "iteration 10400 / 150000: loss 38.905832\n",
      "iteration 10500 / 150000: loss 38.889511\n",
      "iteration 10600 / 150000: loss 38.873342\n",
      "iteration 10700 / 150000: loss 38.857323\n",
      "iteration 10800 / 150000: loss 38.841452\n",
      "iteration 10900 / 150000: loss 38.825727\n",
      "iteration 11000 / 150000: loss 38.810146\n",
      "iteration 11100 / 150000: loss 38.794708\n",
      "iteration 11200 / 150000: loss 38.779411\n",
      "iteration 11300 / 150000: loss 38.764252\n",
      "iteration 11400 / 150000: loss 38.749231\n",
      "iteration 11500 / 150000: loss 38.734345\n",
      "iteration 11600 / 150000: loss 38.719592\n",
      "iteration 11700 / 150000: loss 38.704972\n",
      "iteration 11800 / 150000: loss 38.690482\n",
      "iteration 11900 / 150000: loss 38.676121\n",
      "iteration 12000 / 150000: loss 38.661887\n",
      "iteration 12100 / 150000: loss 38.647779\n",
      "iteration 12200 / 150000: loss 38.633795\n",
      "iteration 12300 / 150000: loss 38.619933\n",
      "iteration 12400 / 150000: loss 38.606193\n",
      "iteration 12500 / 150000: loss 38.592572\n",
      "iteration 12600 / 150000: loss 38.579069\n",
      "iteration 12700 / 150000: loss 38.565683\n",
      "iteration 12800 / 150000: loss 38.552412\n",
      "iteration 12900 / 150000: loss 38.539255\n",
      "iteration 13000 / 150000: loss 38.526211\n",
      "iteration 13100 / 150000: loss 38.513278\n",
      "iteration 13200 / 150000: loss 38.500455\n",
      "iteration 13300 / 150000: loss 38.487740\n",
      "iteration 13400 / 150000: loss 38.475132\n",
      "iteration 13500 / 150000: loss 38.462631\n",
      "iteration 13600 / 150000: loss 38.450234\n",
      "iteration 13700 / 150000: loss 38.437940\n",
      "iteration 13800 / 150000: loss 38.425749\n",
      "iteration 13900 / 150000: loss 38.413659\n",
      "iteration 14000 / 150000: loss 38.401669\n",
      "iteration 14100 / 150000: loss 38.389778\n",
      "iteration 14200 / 150000: loss 38.377984\n",
      "iteration 14300 / 150000: loss 38.366287\n",
      "iteration 14400 / 150000: loss 38.354685\n",
      "iteration 14500 / 150000: loss 38.343177\n",
      "iteration 14600 / 150000: loss 38.331763\n",
      "iteration 14700 / 150000: loss 38.320440\n",
      "iteration 14800 / 150000: loss 38.309209\n",
      "iteration 14900 / 150000: loss 38.298067\n",
      "iteration 15000 / 150000: loss 38.287015\n",
      "iteration 15100 / 150000: loss 38.276050\n",
      "iteration 15200 / 150000: loss 38.265173\n",
      "iteration 15300 / 150000: loss 38.254381\n",
      "iteration 15400 / 150000: loss 38.243675\n",
      "iteration 15500 / 150000: loss 38.233053\n",
      "iteration 15600 / 150000: loss 38.222513\n",
      "iteration 15700 / 150000: loss 38.212056\n",
      "iteration 15800 / 150000: loss 38.201681\n",
      "iteration 15900 / 150000: loss 38.191386\n",
      "iteration 16000 / 150000: loss 38.181170\n",
      "iteration 16100 / 150000: loss 38.171033\n",
      "iteration 16200 / 150000: loss 38.160974\n",
      "iteration 16300 / 150000: loss 38.150991\n",
      "iteration 16400 / 150000: loss 38.141085\n",
      "iteration 16500 / 150000: loss 38.131254\n",
      "iteration 16600 / 150000: loss 38.121498\n",
      "iteration 16700 / 150000: loss 38.111815\n",
      "iteration 16800 / 150000: loss 38.102205\n",
      "iteration 16900 / 150000: loss 38.092668\n",
      "iteration 17000 / 150000: loss 38.083201\n",
      "iteration 17100 / 150000: loss 38.073805\n",
      "iteration 17200 / 150000: loss 38.064479\n",
      "iteration 17300 / 150000: loss 38.055222\n",
      "iteration 17400 / 150000: loss 38.046033\n",
      "iteration 17500 / 150000: loss 38.036912\n",
      "iteration 17600 / 150000: loss 38.027858\n",
      "iteration 17700 / 150000: loss 38.018870\n",
      "iteration 17800 / 150000: loss 38.009948\n",
      "iteration 17900 / 150000: loss 38.001090\n",
      "iteration 18000 / 150000: loss 37.992297\n",
      "iteration 18100 / 150000: loss 37.983567\n",
      "iteration 18200 / 150000: loss 37.974900\n",
      "iteration 18300 / 150000: loss 37.966295\n",
      "iteration 18400 / 150000: loss 37.957752\n",
      "iteration 18500 / 150000: loss 37.949269\n",
      "iteration 18600 / 150000: loss 37.940847\n",
      "iteration 18700 / 150000: loss 37.932485\n",
      "iteration 18800 / 150000: loss 37.924181\n",
      "iteration 18900 / 150000: loss 37.915937\n",
      "iteration 19000 / 150000: loss 37.907750\n",
      "iteration 19100 / 150000: loss 37.899620\n",
      "iteration 19200 / 150000: loss 37.891547\n",
      "iteration 19300 / 150000: loss 37.883531\n",
      "iteration 19400 / 150000: loss 37.875569\n",
      "iteration 19500 / 150000: loss 37.867663\n",
      "iteration 19600 / 150000: loss 37.859812\n",
      "iteration 19700 / 150000: loss 37.852014\n",
      "iteration 19800 / 150000: loss 37.844270\n",
      "iteration 19900 / 150000: loss 37.836579\n",
      "iteration 20000 / 150000: loss 37.828941\n",
      "iteration 20100 / 150000: loss 37.821354\n",
      "iteration 20200 / 150000: loss 37.813818\n",
      "iteration 20300 / 150000: loss 37.806334\n",
      "iteration 20400 / 150000: loss 37.798900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 20500 / 150000: loss 37.791515\n",
      "iteration 20600 / 150000: loss 37.784181\n",
      "iteration 20700 / 150000: loss 37.776895\n",
      "iteration 20800 / 150000: loss 37.769657\n",
      "iteration 20900 / 150000: loss 37.762468\n",
      "iteration 21000 / 150000: loss 37.755326\n",
      "iteration 21100 / 150000: loss 37.748232\n",
      "iteration 21200 / 150000: loss 37.741184\n",
      "iteration 21300 / 150000: loss 37.734182\n",
      "iteration 21400 / 150000: loss 37.727226\n",
      "iteration 21500 / 150000: loss 37.720316\n",
      "iteration 21600 / 150000: loss 37.713450\n",
      "iteration 21700 / 150000: loss 37.706629\n",
      "iteration 21800 / 150000: loss 37.699852\n",
      "iteration 21900 / 150000: loss 37.693119\n",
      "iteration 22000 / 150000: loss 37.686429\n",
      "iteration 22100 / 150000: loss 37.679782\n",
      "iteration 22200 / 150000: loss 37.673178\n",
      "iteration 22300 / 150000: loss 37.666615\n",
      "iteration 22400 / 150000: loss 37.660095\n",
      "iteration 22500 / 150000: loss 37.653615\n",
      "iteration 22600 / 150000: loss 37.647177\n",
      "iteration 22700 / 150000: loss 37.640780\n",
      "iteration 22800 / 150000: loss 37.634422\n",
      "iteration 22900 / 150000: loss 37.628105\n",
      "iteration 23000 / 150000: loss 37.621827\n",
      "iteration 23100 / 150000: loss 37.615588\n",
      "iteration 23200 / 150000: loss 37.609388\n",
      "iteration 23300 / 150000: loss 37.603226\n",
      "iteration 23400 / 150000: loss 37.597103\n",
      "iteration 23500 / 150000: loss 37.591017\n",
      "iteration 23600 / 150000: loss 37.584969\n",
      "iteration 23700 / 150000: loss 37.578958\n",
      "iteration 23800 / 150000: loss 37.572984\n",
      "iteration 23900 / 150000: loss 37.567046\n",
      "iteration 24000 / 150000: loss 37.561145\n",
      "iteration 24100 / 150000: loss 37.555279\n",
      "iteration 24200 / 150000: loss 37.549449\n",
      "iteration 24300 / 150000: loss 37.543654\n",
      "iteration 24400 / 150000: loss 37.537894\n",
      "iteration 24500 / 150000: loss 37.532169\n",
      "iteration 24600 / 150000: loss 37.526478\n",
      "iteration 24700 / 150000: loss 37.520821\n",
      "iteration 24800 / 150000: loss 37.515198\n",
      "iteration 24900 / 150000: loss 37.509608\n",
      "iteration 25000 / 150000: loss 37.504051\n",
      "iteration 25100 / 150000: loss 37.498528\n",
      "iteration 25200 / 150000: loss 37.493037\n",
      "iteration 25300 / 150000: loss 37.487578\n",
      "iteration 25400 / 150000: loss 37.482152\n",
      "iteration 25500 / 150000: loss 37.476757\n",
      "iteration 25600 / 150000: loss 37.471393\n",
      "iteration 25700 / 150000: loss 37.466061\n",
      "iteration 25800 / 150000: loss 37.460760\n",
      "iteration 25900 / 150000: loss 37.455490\n",
      "iteration 26000 / 150000: loss 37.450251\n",
      "iteration 26100 / 150000: loss 37.445041\n",
      "iteration 26200 / 150000: loss 37.439862\n",
      "iteration 26300 / 150000: loss 37.434712\n",
      "iteration 26400 / 150000: loss 37.429592\n",
      "iteration 26500 / 150000: loss 37.424501\n",
      "iteration 26600 / 150000: loss 37.419439\n",
      "iteration 26700 / 150000: loss 37.414406\n",
      "iteration 26800 / 150000: loss 37.409402\n",
      "iteration 26900 / 150000: loss 37.404426\n",
      "iteration 27000 / 150000: loss 37.399477\n",
      "iteration 27100 / 150000: loss 37.394557\n",
      "iteration 27200 / 150000: loss 37.389665\n",
      "iteration 27300 / 150000: loss 37.384800\n",
      "iteration 27400 / 150000: loss 37.379962\n",
      "iteration 27500 / 150000: loss 37.375151\n",
      "iteration 27600 / 150000: loss 37.370367\n",
      "iteration 27700 / 150000: loss 37.365610\n",
      "iteration 27800 / 150000: loss 37.360879\n",
      "iteration 27900 / 150000: loss 37.356174\n",
      "iteration 28000 / 150000: loss 37.351495\n",
      "iteration 28100 / 150000: loss 37.346842\n",
      "iteration 28200 / 150000: loss 37.342214\n",
      "iteration 28300 / 150000: loss 37.337612\n",
      "iteration 28400 / 150000: loss 37.333035\n",
      "iteration 28500 / 150000: loss 37.328483\n",
      "iteration 28600 / 150000: loss 37.323956\n",
      "iteration 28700 / 150000: loss 37.319453\n",
      "iteration 28800 / 150000: loss 37.314974\n",
      "iteration 28900 / 150000: loss 37.310520\n",
      "iteration 29000 / 150000: loss 37.306090\n",
      "iteration 29100 / 150000: loss 37.301684\n",
      "iteration 29200 / 150000: loss 37.297301\n",
      "iteration 29300 / 150000: loss 37.292942\n",
      "iteration 29400 / 150000: loss 37.288606\n",
      "iteration 29500 / 150000: loss 37.284294\n",
      "iteration 29600 / 150000: loss 37.280004\n",
      "iteration 29700 / 150000: loss 37.275737\n",
      "iteration 29800 / 150000: loss 37.271492\n",
      "iteration 29900 / 150000: loss 37.267270\n",
      "iteration 30000 / 150000: loss 37.263071\n",
      "iteration 30100 / 150000: loss 37.258893\n",
      "iteration 30200 / 150000: loss 37.254738\n",
      "iteration 30300 / 150000: loss 37.250604\n",
      "iteration 30400 / 150000: loss 37.246492\n",
      "iteration 30500 / 150000: loss 37.242401\n",
      "iteration 30600 / 150000: loss 37.238331\n",
      "iteration 30700 / 150000: loss 37.234283\n",
      "iteration 30800 / 150000: loss 37.230256\n",
      "iteration 30900 / 150000: loss 37.226249\n",
      "iteration 31000 / 150000: loss 37.222264\n",
      "iteration 31100 / 150000: loss 37.218298\n",
      "iteration 31200 / 150000: loss 37.214353\n",
      "iteration 31300 / 150000: loss 37.210429\n",
      "iteration 31400 / 150000: loss 37.206524\n",
      "iteration 31500 / 150000: loss 37.202640\n",
      "iteration 31600 / 150000: loss 37.198775\n",
      "iteration 31700 / 150000: loss 37.194930\n",
      "iteration 31800 / 150000: loss 37.191104\n",
      "iteration 31900 / 150000: loss 37.187298\n",
      "iteration 32000 / 150000: loss 37.183511\n",
      "iteration 32100 / 150000: loss 37.179744\n",
      "iteration 32200 / 150000: loss 37.175995\n",
      "iteration 32300 / 150000: loss 37.172265\n",
      "iteration 32400 / 150000: loss 37.168553\n",
      "iteration 32500 / 150000: loss 37.164861\n",
      "iteration 32600 / 150000: loss 37.161187\n",
      "iteration 32700 / 150000: loss 37.157531\n",
      "iteration 32800 / 150000: loss 37.153893\n",
      "iteration 32900 / 150000: loss 37.150273\n",
      "iteration 33000 / 150000: loss 37.146672\n",
      "iteration 33100 / 150000: loss 37.143088\n",
      "iteration 33200 / 150000: loss 37.139522\n",
      "iteration 33300 / 150000: loss 37.135973\n",
      "iteration 33400 / 150000: loss 37.132442\n",
      "iteration 33500 / 150000: loss 37.128928\n",
      "iteration 33600 / 150000: loss 37.125431\n",
      "iteration 33700 / 150000: loss 37.121952\n",
      "iteration 33800 / 150000: loss 37.118489\n",
      "iteration 33900 / 150000: loss 37.115044\n",
      "iteration 34000 / 150000: loss 37.111615\n",
      "iteration 34100 / 150000: loss 37.108202\n",
      "iteration 34200 / 150000: loss 37.104807\n",
      "iteration 34300 / 150000: loss 37.101427\n",
      "iteration 34400 / 150000: loss 37.098064\n",
      "iteration 34500 / 150000: loss 37.094717\n",
      "iteration 34600 / 150000: loss 37.091387\n",
      "iteration 34700 / 150000: loss 37.088072\n",
      "iteration 34800 / 150000: loss 37.084773\n",
      "iteration 34900 / 150000: loss 37.081490\n",
      "iteration 35000 / 150000: loss 37.078222\n",
      "iteration 35100 / 150000: loss 37.074971\n",
      "iteration 35200 / 150000: loss 37.071734\n",
      "iteration 35300 / 150000: loss 37.068513\n",
      "iteration 35400 / 150000: loss 37.065307\n",
      "iteration 35500 / 150000: loss 37.062116\n",
      "iteration 35600 / 150000: loss 37.058941\n",
      "iteration 35700 / 150000: loss 37.055780\n",
      "iteration 35800 / 150000: loss 37.052634\n",
      "iteration 35900 / 150000: loss 37.049503\n",
      "iteration 36000 / 150000: loss 37.046387\n",
      "iteration 36100 / 150000: loss 37.043285\n",
      "iteration 36200 / 150000: loss 37.040198\n",
      "iteration 36300 / 150000: loss 37.037125\n",
      "iteration 36400 / 150000: loss 37.034066\n",
      "iteration 36500 / 150000: loss 37.031022\n",
      "iteration 36600 / 150000: loss 37.027991\n",
      "iteration 36700 / 150000: loss 37.024975\n",
      "iteration 36800 / 150000: loss 37.021973\n",
      "iteration 36900 / 150000: loss 37.018984\n",
      "iteration 37000 / 150000: loss 37.016009\n",
      "iteration 37100 / 150000: loss 37.013048\n",
      "iteration 37200 / 150000: loss 37.010100\n",
      "iteration 37300 / 150000: loss 37.007166\n",
      "iteration 37400 / 150000: loss 37.004246\n",
      "iteration 37500 / 150000: loss 37.001338\n",
      "iteration 37600 / 150000: loss 36.998444\n",
      "iteration 37700 / 150000: loss 36.995563\n",
      "iteration 37800 / 150000: loss 36.992696\n",
      "iteration 37900 / 150000: loss 36.989841\n",
      "iteration 38000 / 150000: loss 36.986999\n",
      "iteration 38100 / 150000: loss 36.984170\n",
      "iteration 38200 / 150000: loss 36.981353\n",
      "iteration 38300 / 150000: loss 36.978550\n",
      "iteration 38400 / 150000: loss 36.975759\n",
      "iteration 38500 / 150000: loss 36.972980\n",
      "iteration 38600 / 150000: loss 36.970214\n",
      "iteration 38700 / 150000: loss 36.967460\n",
      "iteration 38800 / 150000: loss 36.964719\n",
      "iteration 38900 / 150000: loss 36.961990\n",
      "iteration 39000 / 150000: loss 36.959272\n",
      "iteration 39100 / 150000: loss 36.956567\n",
      "iteration 39200 / 150000: loss 36.953874\n",
      "iteration 39300 / 150000: loss 36.951193\n",
      "iteration 39400 / 150000: loss 36.948524\n",
      "iteration 39500 / 150000: loss 36.945867\n",
      "iteration 39600 / 150000: loss 36.943221\n",
      "iteration 39700 / 150000: loss 36.940587\n",
      "iteration 39800 / 150000: loss 36.937964\n",
      "iteration 39900 / 150000: loss 36.935353\n",
      "iteration 40000 / 150000: loss 36.932754\n",
      "iteration 40100 / 150000: loss 36.930165\n",
      "iteration 40200 / 150000: loss 36.927588\n",
      "iteration 40300 / 150000: loss 36.925023\n",
      "iteration 40400 / 150000: loss 36.922468\n",
      "iteration 40500 / 150000: loss 36.919925\n",
      "iteration 40600 / 150000: loss 36.917392\n",
      "iteration 40700 / 150000: loss 36.914871\n",
      "iteration 40800 / 150000: loss 36.912360\n",
      "iteration 40900 / 150000: loss 36.909861\n",
      "iteration 41000 / 150000: loss 36.907372\n",
      "iteration 41100 / 150000: loss 36.904893\n",
      "iteration 41200 / 150000: loss 36.902426\n",
      "iteration 41300 / 150000: loss 36.899969\n",
      "iteration 41400 / 150000: loss 36.897523\n",
      "iteration 41500 / 150000: loss 36.895087\n",
      "iteration 41600 / 150000: loss 36.892661\n",
      "iteration 41700 / 150000: loss 36.890246\n",
      "iteration 41800 / 150000: loss 36.887841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 41900 / 150000: loss 36.885446\n",
      "iteration 42000 / 150000: loss 36.883062\n",
      "iteration 42100 / 150000: loss 36.880687\n",
      "iteration 42200 / 150000: loss 36.878323\n",
      "iteration 42300 / 150000: loss 36.875968\n",
      "iteration 42400 / 150000: loss 36.873624\n",
      "iteration 42500 / 150000: loss 36.871289\n",
      "iteration 42600 / 150000: loss 36.868965\n",
      "iteration 42700 / 150000: loss 36.866650\n",
      "iteration 42800 / 150000: loss 36.864344\n",
      "iteration 42900 / 150000: loss 36.862049\n",
      "iteration 43000 / 150000: loss 36.859763\n",
      "iteration 43100 / 150000: loss 36.857487\n",
      "iteration 43200 / 150000: loss 36.855220\n",
      "iteration 43300 / 150000: loss 36.852962\n",
      "iteration 43400 / 150000: loss 36.850714\n",
      "iteration 43500 / 150000: loss 36.848475\n",
      "iteration 43600 / 150000: loss 36.846246\n",
      "iteration 43700 / 150000: loss 36.844026\n",
      "iteration 43800 / 150000: loss 36.841815\n",
      "iteration 43900 / 150000: loss 36.839613\n",
      "iteration 44000 / 150000: loss 36.837420\n",
      "iteration 44100 / 150000: loss 36.835236\n",
      "iteration 44200 / 150000: loss 36.833062\n",
      "iteration 44300 / 150000: loss 36.830896\n",
      "iteration 44400 / 150000: loss 36.828739\n",
      "iteration 44500 / 150000: loss 36.826591\n",
      "iteration 44600 / 150000: loss 36.824451\n",
      "iteration 44700 / 150000: loss 36.822321\n",
      "iteration 44800 / 150000: loss 36.820199\n",
      "iteration 44900 / 150000: loss 36.818086\n",
      "iteration 45000 / 150000: loss 36.815981\n",
      "iteration 45100 / 150000: loss 36.813885\n",
      "iteration 45200 / 150000: loss 36.811798\n",
      "iteration 45300 / 150000: loss 36.809719\n",
      "iteration 45400 / 150000: loss 36.807648\n",
      "iteration 45500 / 150000: loss 36.805586\n",
      "iteration 45600 / 150000: loss 36.803532\n",
      "iteration 45700 / 150000: loss 36.801486\n",
      "iteration 45800 / 150000: loss 36.799449\n",
      "iteration 45900 / 150000: loss 36.797420\n",
      "iteration 46000 / 150000: loss 36.795398\n",
      "iteration 46100 / 150000: loss 36.793386\n",
      "iteration 46200 / 150000: loss 36.791381\n",
      "iteration 46300 / 150000: loss 36.789384\n",
      "iteration 46400 / 150000: loss 36.787395\n",
      "iteration 46500 / 150000: loss 36.785414\n",
      "iteration 46600 / 150000: loss 36.783441\n",
      "iteration 46700 / 150000: loss 36.781476\n",
      "iteration 46800 / 150000: loss 36.779518\n",
      "iteration 46900 / 150000: loss 36.777569\n",
      "iteration 47000 / 150000: loss 36.775627\n",
      "iteration 47100 / 150000: loss 36.773693\n",
      "iteration 47200 / 150000: loss 36.771766\n",
      "iteration 47300 / 150000: loss 36.769848\n",
      "iteration 47400 / 150000: loss 36.767936\n",
      "iteration 47500 / 150000: loss 36.766032\n",
      "iteration 47600 / 150000: loss 36.764136\n",
      "iteration 47700 / 150000: loss 36.762247\n",
      "iteration 47800 / 150000: loss 36.760366\n",
      "iteration 47900 / 150000: loss 36.758492\n",
      "iteration 48000 / 150000: loss 36.756625\n",
      "iteration 48100 / 150000: loss 36.754766\n",
      "iteration 48200 / 150000: loss 36.752914\n",
      "iteration 48300 / 150000: loss 36.751069\n",
      "iteration 48400 / 150000: loss 36.749232\n",
      "iteration 48500 / 150000: loss 36.747401\n",
      "iteration 48600 / 150000: loss 36.745578\n",
      "iteration 48700 / 150000: loss 36.743761\n",
      "iteration 48800 / 150000: loss 36.741952\n",
      "iteration 48900 / 150000: loss 36.740150\n",
      "iteration 49000 / 150000: loss 36.738355\n",
      "iteration 49100 / 150000: loss 36.736567\n",
      "iteration 49200 / 150000: loss 36.734785\n",
      "iteration 49300 / 150000: loss 36.733011\n",
      "iteration 49400 / 150000: loss 36.731243\n",
      "iteration 49500 / 150000: loss 36.729482\n",
      "iteration 49600 / 150000: loss 36.727728\n",
      "iteration 49700 / 150000: loss 36.725981\n",
      "iteration 49800 / 150000: loss 36.724240\n",
      "iteration 49900 / 150000: loss 36.722506\n",
      "iteration 50000 / 150000: loss 36.720779\n",
      "iteration 50100 / 150000: loss 36.719059\n",
      "iteration 50200 / 150000: loss 36.717344\n",
      "iteration 50300 / 150000: loss 36.715637\n",
      "iteration 50400 / 150000: loss 36.713936\n",
      "iteration 50500 / 150000: loss 36.712241\n",
      "iteration 50600 / 150000: loss 36.710553\n",
      "iteration 50700 / 150000: loss 36.708872\n",
      "iteration 50800 / 150000: loss 36.707196\n",
      "iteration 50900 / 150000: loss 36.705527\n",
      "iteration 51000 / 150000: loss 36.703865\n",
      "iteration 51100 / 150000: loss 36.702208\n",
      "iteration 51200 / 150000: loss 36.700558\n",
      "iteration 51300 / 150000: loss 36.698914\n",
      "iteration 51400 / 150000: loss 36.697277\n",
      "iteration 51500 / 150000: loss 36.695645\n",
      "iteration 51600 / 150000: loss 36.694020\n",
      "iteration 51700 / 150000: loss 36.692401\n",
      "iteration 51800 / 150000: loss 36.690788\n",
      "iteration 51900 / 150000: loss 36.689181\n",
      "iteration 52000 / 150000: loss 36.687580\n",
      "iteration 52100 / 150000: loss 36.685985\n",
      "iteration 52200 / 150000: loss 36.684396\n",
      "iteration 52300 / 150000: loss 36.682812\n",
      "iteration 52400 / 150000: loss 36.681235\n",
      "iteration 52500 / 150000: loss 36.679664\n",
      "iteration 52600 / 150000: loss 36.678098\n",
      "iteration 52700 / 150000: loss 36.676539\n",
      "iteration 52800 / 150000: loss 36.674985\n",
      "iteration 52900 / 150000: loss 36.673437\n",
      "iteration 53000 / 150000: loss 36.671894\n",
      "iteration 53100 / 150000: loss 36.670358\n",
      "iteration 53200 / 150000: loss 36.668827\n",
      "iteration 53300 / 150000: loss 36.667302\n",
      "iteration 53400 / 150000: loss 36.665782\n",
      "iteration 53500 / 150000: loss 36.664268\n",
      "iteration 53600 / 150000: loss 36.662760\n",
      "iteration 53700 / 150000: loss 36.661257\n",
      "iteration 53800 / 150000: loss 36.659759\n",
      "iteration 53900 / 150000: loss 36.658268\n",
      "iteration 54000 / 150000: loss 36.656781\n",
      "iteration 54100 / 150000: loss 36.655301\n",
      "iteration 54200 / 150000: loss 36.653825\n",
      "iteration 54300 / 150000: loss 36.652355\n",
      "iteration 54400 / 150000: loss 36.650890\n",
      "iteration 54500 / 150000: loss 36.649431\n",
      "iteration 54600 / 150000: loss 36.647977\n",
      "iteration 54700 / 150000: loss 36.646529\n",
      "iteration 54800 / 150000: loss 36.645085\n",
      "iteration 54900 / 150000: loss 36.643647\n",
      "iteration 55000 / 150000: loss 36.642214\n",
      "iteration 55100 / 150000: loss 36.640787\n",
      "iteration 55200 / 150000: loss 36.639364\n",
      "iteration 55300 / 150000: loss 36.637947\n",
      "iteration 55400 / 150000: loss 36.636535\n",
      "iteration 55500 / 150000: loss 36.635128\n",
      "iteration 55600 / 150000: loss 36.633726\n",
      "iteration 55700 / 150000: loss 36.632329\n",
      "iteration 55800 / 150000: loss 36.630937\n",
      "iteration 55900 / 150000: loss 36.629551\n",
      "iteration 56000 / 150000: loss 36.628169\n",
      "iteration 56100 / 150000: loss 36.626792\n",
      "iteration 56200 / 150000: loss 36.625420\n",
      "iteration 56300 / 150000: loss 36.624053\n",
      "iteration 56400 / 150000: loss 36.622692\n",
      "iteration 56500 / 150000: loss 36.621334\n",
      "iteration 56600 / 150000: loss 36.619982\n",
      "iteration 56700 / 150000: loss 36.618635\n",
      "iteration 56800 / 150000: loss 36.617293\n",
      "iteration 56900 / 150000: loss 36.615955\n",
      "iteration 57000 / 150000: loss 36.614622\n",
      "iteration 57100 / 150000: loss 36.613294\n",
      "iteration 57200 / 150000: loss 36.611970\n",
      "iteration 57300 / 150000: loss 36.610652\n",
      "iteration 57400 / 150000: loss 36.609338\n",
      "iteration 57500 / 150000: loss 36.608029\n",
      "iteration 57600 / 150000: loss 36.606724\n",
      "iteration 57700 / 150000: loss 36.605424\n",
      "iteration 57800 / 150000: loss 36.604129\n",
      "iteration 57900 / 150000: loss 36.602838\n",
      "iteration 58000 / 150000: loss 36.601552\n",
      "iteration 58100 / 150000: loss 36.600270\n",
      "iteration 58200 / 150000: loss 36.598993\n",
      "iteration 58300 / 150000: loss 36.597721\n",
      "iteration 58400 / 150000: loss 36.596453\n",
      "iteration 58500 / 150000: loss 36.595189\n",
      "iteration 58600 / 150000: loss 36.593930\n",
      "iteration 58700 / 150000: loss 36.592675\n",
      "iteration 58800 / 150000: loss 36.591425\n",
      "iteration 58900 / 150000: loss 36.590179\n",
      "iteration 59000 / 150000: loss 36.588938\n",
      "iteration 59100 / 150000: loss 36.587701\n",
      "iteration 59200 / 150000: loss 36.586468\n",
      "iteration 59300 / 150000: loss 36.585240\n",
      "iteration 59400 / 150000: loss 36.584016\n",
      "iteration 59500 / 150000: loss 36.582796\n",
      "iteration 59600 / 150000: loss 36.581581\n",
      "iteration 59700 / 150000: loss 36.580369\n",
      "iteration 59800 / 150000: loss 36.579163\n",
      "iteration 59900 / 150000: loss 36.577960\n",
      "iteration 60000 / 150000: loss 36.576761\n",
      "iteration 60100 / 150000: loss 36.575567\n",
      "iteration 60200 / 150000: loss 36.574377\n",
      "iteration 60300 / 150000: loss 36.573191\n",
      "iteration 60400 / 150000: loss 36.572009\n",
      "iteration 60500 / 150000: loss 36.570831\n",
      "iteration 60600 / 150000: loss 36.569657\n",
      "iteration 60700 / 150000: loss 36.568488\n",
      "iteration 60800 / 150000: loss 36.567322\n",
      "iteration 60900 / 150000: loss 36.566161\n",
      "iteration 61000 / 150000: loss 36.565003\n",
      "iteration 61100 / 150000: loss 36.563850\n",
      "iteration 61200 / 150000: loss 36.562700\n",
      "iteration 61300 / 150000: loss 36.561555\n",
      "iteration 61400 / 150000: loss 36.560413\n",
      "iteration 61500 / 150000: loss 36.559276\n",
      "iteration 61600 / 150000: loss 36.558142\n",
      "iteration 61700 / 150000: loss 36.557013\n",
      "iteration 61800 / 150000: loss 36.555887\n",
      "iteration 61900 / 150000: loss 36.554765\n",
      "iteration 62000 / 150000: loss 36.553647\n",
      "iteration 62100 / 150000: loss 36.552533\n",
      "iteration 62200 / 150000: loss 36.551422\n",
      "iteration 62300 / 150000: loss 36.550316\n",
      "iteration 62400 / 150000: loss 36.549213\n",
      "iteration 62500 / 150000: loss 36.548114\n",
      "iteration 62600 / 150000: loss 36.547019\n",
      "iteration 62700 / 150000: loss 36.545928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 62800 / 150000: loss 36.544840\n",
      "iteration 62900 / 150000: loss 36.543756\n",
      "iteration 63000 / 150000: loss 36.542676\n",
      "iteration 63100 / 150000: loss 36.541599\n",
      "iteration 63200 / 150000: loss 36.540526\n",
      "iteration 63300 / 150000: loss 36.539457\n",
      "iteration 63400 / 150000: loss 36.538392\n",
      "iteration 63500 / 150000: loss 36.537330\n",
      "iteration 63600 / 150000: loss 36.536272\n",
      "iteration 63700 / 150000: loss 36.535217\n",
      "iteration 63800 / 150000: loss 36.534166\n",
      "iteration 63900 / 150000: loss 36.533118\n",
      "iteration 64000 / 150000: loss 36.532074\n",
      "iteration 64100 / 150000: loss 36.531034\n",
      "iteration 64200 / 150000: loss 36.529997\n",
      "iteration 64300 / 150000: loss 36.528964\n",
      "iteration 64400 / 150000: loss 36.527934\n",
      "iteration 64500 / 150000: loss 36.526908\n",
      "iteration 64600 / 150000: loss 36.525885\n",
      "iteration 64700 / 150000: loss 36.524866\n",
      "iteration 64800 / 150000: loss 36.523850\n",
      "iteration 64900 / 150000: loss 36.522837\n",
      "iteration 65000 / 150000: loss 36.521828\n",
      "iteration 65100 / 150000: loss 36.520823\n",
      "iteration 65200 / 150000: loss 36.519820\n",
      "iteration 65300 / 150000: loss 36.518822\n",
      "iteration 65400 / 150000: loss 36.517826\n",
      "iteration 65500 / 150000: loss 36.516834\n",
      "iteration 65600 / 150000: loss 36.515845\n",
      "iteration 65700 / 150000: loss 36.514860\n",
      "iteration 65800 / 150000: loss 36.513878\n",
      "iteration 65900 / 150000: loss 36.512899\n",
      "iteration 66000 / 150000: loss 36.511923\n",
      "iteration 66100 / 150000: loss 36.510951\n",
      "iteration 66200 / 150000: loss 36.509982\n",
      "iteration 66300 / 150000: loss 36.509016\n",
      "iteration 66400 / 150000: loss 36.508054\n",
      "iteration 66500 / 150000: loss 36.507095\n",
      "iteration 66600 / 150000: loss 36.506139\n",
      "iteration 66700 / 150000: loss 36.505186\n",
      "iteration 66800 / 150000: loss 36.504236\n",
      "iteration 66900 / 150000: loss 36.503290\n",
      "iteration 67000 / 150000: loss 36.502346\n",
      "iteration 67100 / 150000: loss 36.501406\n",
      "iteration 67200 / 150000: loss 36.500469\n",
      "iteration 67300 / 150000: loss 36.499535\n",
      "iteration 67400 / 150000: loss 36.498605\n",
      "iteration 67500 / 150000: loss 36.497677\n",
      "iteration 67600 / 150000: loss 36.496752\n",
      "iteration 67700 / 150000: loss 36.495831\n",
      "iteration 67800 / 150000: loss 36.494912\n",
      "iteration 67900 / 150000: loss 36.493997\n",
      "iteration 68000 / 150000: loss 36.493085\n",
      "iteration 68100 / 150000: loss 36.492176\n",
      "iteration 68200 / 150000: loss 36.491269\n",
      "iteration 68300 / 150000: loss 36.490366\n",
      "iteration 68400 / 150000: loss 36.489466\n",
      "iteration 68500 / 150000: loss 36.488569\n",
      "iteration 68600 / 150000: loss 36.487674\n",
      "iteration 68700 / 150000: loss 36.486783\n",
      "iteration 68800 / 150000: loss 36.485895\n",
      "iteration 68900 / 150000: loss 36.485009\n",
      "iteration 69000 / 150000: loss 36.484127\n",
      "iteration 69100 / 150000: loss 36.483247\n",
      "iteration 69200 / 150000: loss 36.482371\n",
      "iteration 69300 / 150000: loss 36.481497\n",
      "iteration 69400 / 150000: loss 36.480626\n",
      "iteration 69500 / 150000: loss 36.479758\n",
      "iteration 69600 / 150000: loss 36.478893\n",
      "iteration 69700 / 150000: loss 36.478031\n",
      "iteration 69800 / 150000: loss 36.477171\n",
      "iteration 69900 / 150000: loss 36.476315\n",
      "iteration 70000 / 150000: loss 36.475461\n",
      "iteration 70100 / 150000: loss 36.474610\n",
      "iteration 70200 / 150000: loss 36.473762\n",
      "iteration 70300 / 150000: loss 36.472917\n",
      "iteration 70400 / 150000: loss 36.472074\n",
      "iteration 70500 / 150000: loss 36.471234\n",
      "iteration 70600 / 150000: loss 36.470397\n",
      "iteration 70700 / 150000: loss 36.469563\n",
      "iteration 70800 / 150000: loss 36.468732\n",
      "iteration 70900 / 150000: loss 36.467903\n",
      "iteration 71000 / 150000: loss 36.467077\n",
      "iteration 71100 / 150000: loss 36.466253\n",
      "iteration 71200 / 150000: loss 36.465433\n",
      "iteration 71300 / 150000: loss 36.464615\n",
      "iteration 71400 / 150000: loss 36.463799\n",
      "iteration 71500 / 150000: loss 36.462987\n",
      "iteration 71600 / 150000: loss 36.462177\n",
      "iteration 71700 / 150000: loss 36.461369\n",
      "iteration 71800 / 150000: loss 36.460565\n",
      "iteration 71900 / 150000: loss 36.459763\n",
      "iteration 72000 / 150000: loss 36.458963\n",
      "iteration 72100 / 150000: loss 36.458166\n",
      "iteration 72200 / 150000: loss 36.457372\n",
      "iteration 72300 / 150000: loss 36.456580\n",
      "iteration 72400 / 150000: loss 36.455791\n",
      "iteration 72500 / 150000: loss 36.455005\n",
      "iteration 72600 / 150000: loss 36.454221\n",
      "iteration 72700 / 150000: loss 36.453440\n",
      "iteration 72800 / 150000: loss 36.452661\n",
      "iteration 72900 / 150000: loss 36.451885\n",
      "iteration 73000 / 150000: loss 36.451111\n",
      "iteration 73100 / 150000: loss 36.450340\n",
      "iteration 73200 / 150000: loss 36.449571\n",
      "iteration 73300 / 150000: loss 36.448805\n",
      "iteration 73400 / 150000: loss 36.448041\n",
      "iteration 73500 / 150000: loss 36.447280\n",
      "iteration 73600 / 150000: loss 36.446521\n",
      "iteration 73700 / 150000: loss 36.445764\n",
      "iteration 73800 / 150000: loss 36.445011\n",
      "iteration 73900 / 150000: loss 36.444259\n",
      "iteration 74000 / 150000: loss 36.443510\n",
      "iteration 74100 / 150000: loss 36.442764\n",
      "iteration 74200 / 150000: loss 36.442019\n",
      "iteration 74300 / 150000: loss 36.441278\n",
      "iteration 74400 / 150000: loss 36.440538\n",
      "iteration 74500 / 150000: loss 36.439801\n",
      "iteration 74600 / 150000: loss 36.439067\n",
      "iteration 74700 / 150000: loss 36.438335\n",
      "iteration 74800 / 150000: loss 36.437605\n",
      "iteration 74900 / 150000: loss 36.436877\n",
      "iteration 75000 / 150000: loss 36.436152\n",
      "iteration 75100 / 150000: loss 36.435429\n",
      "iteration 75200 / 150000: loss 36.434709\n",
      "iteration 75300 / 150000: loss 36.433991\n",
      "iteration 75400 / 150000: loss 36.433275\n",
      "iteration 75500 / 150000: loss 36.432562\n",
      "iteration 75600 / 150000: loss 36.431850\n",
      "iteration 75700 / 150000: loss 36.431141\n",
      "iteration 75800 / 150000: loss 36.430435\n",
      "iteration 75900 / 150000: loss 36.429730\n",
      "iteration 76000 / 150000: loss 36.429028\n",
      "iteration 76100 / 150000: loss 36.428329\n",
      "iteration 76200 / 150000: loss 36.427631\n",
      "iteration 76300 / 150000: loss 36.426936\n",
      "iteration 76400 / 150000: loss 36.426243\n",
      "iteration 76500 / 150000: loss 36.425552\n",
      "iteration 76600 / 150000: loss 36.424863\n",
      "iteration 76700 / 150000: loss 36.424177\n",
      "iteration 76800 / 150000: loss 36.423492\n",
      "iteration 76900 / 150000: loss 36.422810\n",
      "iteration 77000 / 150000: loss 36.422130\n",
      "iteration 77100 / 150000: loss 36.421453\n",
      "iteration 77200 / 150000: loss 36.420777\n",
      "iteration 77300 / 150000: loss 36.420104\n",
      "iteration 77400 / 150000: loss 36.419433\n",
      "iteration 77500 / 150000: loss 36.418764\n",
      "iteration 77600 / 150000: loss 36.418097\n",
      "iteration 77700 / 150000: loss 36.417432\n",
      "iteration 77800 / 150000: loss 36.416769\n",
      "iteration 77900 / 150000: loss 36.416109\n",
      "iteration 78000 / 150000: loss 36.415451\n",
      "iteration 78100 / 150000: loss 36.414794\n",
      "iteration 78200 / 150000: loss 36.414140\n",
      "iteration 78300 / 150000: loss 36.413488\n",
      "iteration 78400 / 150000: loss 36.412838\n",
      "iteration 78500 / 150000: loss 36.412190\n",
      "iteration 78600 / 150000: loss 36.411544\n",
      "iteration 78700 / 150000: loss 36.410900\n",
      "iteration 78800 / 150000: loss 36.410258\n",
      "iteration 78900 / 150000: loss 36.409619\n",
      "iteration 79000 / 150000: loss 36.408981\n",
      "iteration 79100 / 150000: loss 36.408345\n",
      "iteration 79200 / 150000: loss 36.407712\n",
      "iteration 79300 / 150000: loss 36.407080\n",
      "iteration 79400 / 150000: loss 36.406451\n",
      "iteration 79500 / 150000: loss 36.405823\n",
      "iteration 79600 / 150000: loss 36.405197\n",
      "iteration 79700 / 150000: loss 36.404574\n",
      "iteration 79800 / 150000: loss 36.403952\n",
      "iteration 79900 / 150000: loss 36.403332\n",
      "iteration 80000 / 150000: loss 36.402715\n",
      "iteration 80100 / 150000: loss 36.402099\n",
      "iteration 80200 / 150000: loss 36.401485\n",
      "iteration 80300 / 150000: loss 36.400873\n",
      "iteration 80400 / 150000: loss 36.400264\n",
      "iteration 80500 / 150000: loss 36.399656\n",
      "iteration 80600 / 150000: loss 36.399050\n",
      "iteration 80700 / 150000: loss 36.398446\n",
      "iteration 80800 / 150000: loss 36.397843\n",
      "iteration 80900 / 150000: loss 36.397243\n",
      "iteration 81000 / 150000: loss 36.396645\n",
      "iteration 81100 / 150000: loss 36.396048\n",
      "iteration 81200 / 150000: loss 36.395454\n",
      "iteration 81300 / 150000: loss 36.394861\n",
      "iteration 81400 / 150000: loss 36.394270\n",
      "iteration 81500 / 150000: loss 36.393681\n",
      "iteration 81600 / 150000: loss 36.393094\n",
      "iteration 81700 / 150000: loss 36.392509\n",
      "iteration 81800 / 150000: loss 36.391926\n",
      "iteration 81900 / 150000: loss 36.391344\n",
      "iteration 82000 / 150000: loss 36.390764\n",
      "iteration 82100 / 150000: loss 36.390187\n",
      "iteration 82200 / 150000: loss 36.389611\n",
      "iteration 82300 / 150000: loss 36.389036\n",
      "iteration 82400 / 150000: loss 36.388464\n",
      "iteration 82500 / 150000: loss 36.387893\n",
      "iteration 82600 / 150000: loss 36.387325\n",
      "iteration 82700 / 150000: loss 36.386758\n",
      "iteration 82800 / 150000: loss 36.386192\n",
      "iteration 82900 / 150000: loss 36.385629\n",
      "iteration 83000 / 150000: loss 36.385067\n",
      "iteration 83100 / 150000: loss 36.384507\n",
      "iteration 83200 / 150000: loss 36.383949\n",
      "iteration 83300 / 150000: loss 36.383393\n",
      "iteration 83400 / 150000: loss 36.382838\n",
      "iteration 83500 / 150000: loss 36.382285\n",
      "iteration 83600 / 150000: loss 36.381734\n",
      "iteration 83700 / 150000: loss 36.381185\n",
      "iteration 83800 / 150000: loss 36.380637\n",
      "iteration 83900 / 150000: loss 36.380091\n",
      "iteration 84000 / 150000: loss 36.379547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 84100 / 150000: loss 36.379005\n",
      "iteration 84200 / 150000: loss 36.378464\n",
      "iteration 84300 / 150000: loss 36.377925\n",
      "iteration 84400 / 150000: loss 36.377387\n",
      "iteration 84500 / 150000: loss 36.376852\n",
      "iteration 84600 / 150000: loss 36.376318\n",
      "iteration 84700 / 150000: loss 36.375785\n",
      "iteration 84800 / 150000: loss 36.375255\n",
      "iteration 84900 / 150000: loss 36.374726\n",
      "iteration 85000 / 150000: loss 36.374198\n",
      "iteration 85100 / 150000: loss 36.373673\n",
      "iteration 85200 / 150000: loss 36.373148\n",
      "iteration 85300 / 150000: loss 36.372626\n",
      "iteration 85400 / 150000: loss 36.372105\n",
      "iteration 85500 / 150000: loss 36.371586\n",
      "iteration 85600 / 150000: loss 36.371069\n",
      "iteration 85700 / 150000: loss 36.370553\n",
      "iteration 85800 / 150000: loss 36.370039\n",
      "iteration 85900 / 150000: loss 36.369526\n",
      "iteration 86000 / 150000: loss 36.369015\n",
      "iteration 86100 / 150000: loss 36.368505\n",
      "iteration 86200 / 150000: loss 36.367998\n",
      "iteration 86300 / 150000: loss 36.367491\n",
      "iteration 86400 / 150000: loss 36.366987\n",
      "iteration 86500 / 150000: loss 36.366483\n",
      "iteration 86600 / 150000: loss 36.365982\n",
      "iteration 86700 / 150000: loss 36.365482\n",
      "iteration 86800 / 150000: loss 36.364984\n",
      "iteration 86900 / 150000: loss 36.364487\n",
      "iteration 87000 / 150000: loss 36.363991\n",
      "iteration 87100 / 150000: loss 36.363498\n",
      "iteration 87200 / 150000: loss 36.363006\n",
      "iteration 87300 / 150000: loss 36.362515\n",
      "iteration 87400 / 150000: loss 36.362026\n",
      "iteration 87500 / 150000: loss 36.361538\n",
      "iteration 87600 / 150000: loss 36.361052\n",
      "iteration 87700 / 150000: loss 36.360568\n",
      "iteration 87800 / 150000: loss 36.360084\n",
      "iteration 87900 / 150000: loss 36.359603\n",
      "iteration 88000 / 150000: loss 36.359123\n",
      "iteration 88100 / 150000: loss 36.358644\n",
      "iteration 88200 / 150000: loss 36.358167\n",
      "iteration 88300 / 150000: loss 36.357692\n",
      "iteration 88400 / 150000: loss 36.357218\n",
      "iteration 88500 / 150000: loss 36.356745\n",
      "iteration 88600 / 150000: loss 36.356274\n",
      "iteration 88700 / 150000: loss 36.355804\n",
      "iteration 88800 / 150000: loss 36.355336\n",
      "iteration 88900 / 150000: loss 36.354869\n",
      "iteration 89000 / 150000: loss 36.354404\n",
      "iteration 89100 / 150000: loss 36.353940\n",
      "iteration 89200 / 150000: loss 36.353478\n",
      "iteration 89300 / 150000: loss 36.353017\n",
      "iteration 89400 / 150000: loss 36.352557\n",
      "iteration 89500 / 150000: loss 36.352099\n",
      "iteration 89600 / 150000: loss 36.351643\n",
      "iteration 89700 / 150000: loss 36.351187\n",
      "iteration 89800 / 150000: loss 36.350733\n",
      "iteration 89900 / 150000: loss 36.350281\n",
      "iteration 90000 / 150000: loss 36.349830\n",
      "iteration 90100 / 150000: loss 36.349380\n",
      "iteration 90200 / 150000: loss 36.348932\n",
      "iteration 90300 / 150000: loss 36.348485\n",
      "iteration 90400 / 150000: loss 36.348040\n",
      "iteration 90500 / 150000: loss 36.347596\n",
      "iteration 90600 / 150000: loss 36.347153\n",
      "iteration 90700 / 150000: loss 36.346712\n",
      "iteration 90800 / 150000: loss 36.346272\n",
      "iteration 90900 / 150000: loss 36.345833\n",
      "iteration 91000 / 150000: loss 36.345396\n",
      "iteration 91100 / 150000: loss 36.344960\n",
      "iteration 91200 / 150000: loss 36.344526\n",
      "iteration 91300 / 150000: loss 36.344093\n",
      "iteration 91400 / 150000: loss 36.343661\n",
      "iteration 91500 / 150000: loss 36.343231\n",
      "iteration 91600 / 150000: loss 36.342801\n",
      "iteration 91700 / 150000: loss 36.342374\n",
      "iteration 91800 / 150000: loss 36.341947\n",
      "iteration 91900 / 150000: loss 36.341522\n",
      "iteration 92000 / 150000: loss 36.341098\n",
      "iteration 92100 / 150000: loss 36.340676\n",
      "iteration 92200 / 150000: loss 36.340254\n",
      "iteration 92300 / 150000: loss 36.339834\n",
      "iteration 92400 / 150000: loss 36.339416\n",
      "iteration 92500 / 150000: loss 36.338999\n",
      "iteration 92600 / 150000: loss 36.338583\n",
      "iteration 92700 / 150000: loss 36.338168\n",
      "iteration 92800 / 150000: loss 36.337754\n",
      "iteration 92900 / 150000: loss 36.337342\n",
      "iteration 93000 / 150000: loss 36.336931\n",
      "iteration 93100 / 150000: loss 36.336522\n",
      "iteration 93200 / 150000: loss 36.336113\n",
      "iteration 93300 / 150000: loss 36.335706\n",
      "iteration 93400 / 150000: loss 36.335300\n",
      "iteration 93500 / 150000: loss 36.334896\n",
      "iteration 93600 / 150000: loss 36.334492\n",
      "iteration 93700 / 150000: loss 36.334090\n",
      "iteration 93800 / 150000: loss 36.333690\n",
      "iteration 93900 / 150000: loss 36.333290\n",
      "iteration 94000 / 150000: loss 36.332892\n",
      "iteration 94100 / 150000: loss 36.332494\n",
      "iteration 94200 / 150000: loss 36.332098\n",
      "iteration 94300 / 150000: loss 36.331704\n",
      "iteration 94400 / 150000: loss 36.331310\n",
      "iteration 94500 / 150000: loss 36.330918\n",
      "iteration 94600 / 150000: loss 36.330527\n",
      "iteration 94700 / 150000: loss 36.330137\n",
      "iteration 94800 / 150000: loss 36.329749\n",
      "iteration 94900 / 150000: loss 36.329361\n",
      "iteration 95000 / 150000: loss 36.328975\n",
      "iteration 95100 / 150000: loss 36.328590\n",
      "iteration 95200 / 150000: loss 36.328206\n",
      "iteration 95300 / 150000: loss 36.327823\n",
      "iteration 95400 / 150000: loss 36.327442\n",
      "iteration 95500 / 150000: loss 36.327061\n",
      "iteration 95600 / 150000: loss 36.326682\n",
      "iteration 95700 / 150000: loss 36.326304\n",
      "iteration 95800 / 150000: loss 36.325927\n",
      "iteration 95900 / 150000: loss 36.325552\n",
      "iteration 96000 / 150000: loss 36.325177\n",
      "iteration 96100 / 150000: loss 36.324804\n",
      "iteration 96200 / 150000: loss 36.324432\n",
      "iteration 96300 / 150000: loss 36.324061\n",
      "iteration 96400 / 150000: loss 36.323691\n",
      "iteration 96500 / 150000: loss 36.323322\n",
      "iteration 96600 / 150000: loss 36.322954\n",
      "iteration 96700 / 150000: loss 36.322588\n",
      "iteration 96800 / 150000: loss 36.322222\n",
      "iteration 96900 / 150000: loss 36.321858\n",
      "iteration 97000 / 150000: loss 36.321495\n",
      "iteration 97100 / 150000: loss 36.321133\n",
      "iteration 97200 / 150000: loss 36.320772\n",
      "iteration 97300 / 150000: loss 36.320412\n",
      "iteration 97400 / 150000: loss 36.320054\n",
      "iteration 97500 / 150000: loss 36.319696\n",
      "iteration 97600 / 150000: loss 36.319340\n",
      "iteration 97700 / 150000: loss 36.318984\n",
      "iteration 97800 / 150000: loss 36.318630\n",
      "iteration 97900 / 150000: loss 36.318277\n",
      "iteration 98000 / 150000: loss 36.317925\n",
      "iteration 98100 / 150000: loss 36.317574\n",
      "iteration 98200 / 150000: loss 36.317224\n",
      "iteration 98300 / 150000: loss 36.316875\n",
      "iteration 98400 / 150000: loss 36.316527\n",
      "iteration 98500 / 150000: loss 36.316180\n",
      "iteration 98600 / 150000: loss 36.315834\n",
      "iteration 98700 / 150000: loss 36.315490\n",
      "iteration 98800 / 150000: loss 36.315146\n",
      "iteration 98900 / 150000: loss 36.314804\n",
      "iteration 99000 / 150000: loss 36.314462\n",
      "iteration 99100 / 150000: loss 36.314122\n",
      "iteration 99200 / 150000: loss 36.313783\n",
      "iteration 99300 / 150000: loss 36.313444\n",
      "iteration 99400 / 150000: loss 36.313107\n",
      "iteration 99500 / 150000: loss 36.312771\n",
      "iteration 99600 / 150000: loss 36.312435\n",
      "iteration 99700 / 150000: loss 36.312101\n",
      "iteration 99800 / 150000: loss 36.311768\n",
      "iteration 99900 / 150000: loss 36.311436\n",
      "iteration 100000 / 150000: loss 36.311105\n",
      "iteration 100100 / 150000: loss 36.310775\n",
      "iteration 100200 / 150000: loss 36.310446\n",
      "iteration 100300 / 150000: loss 36.310117\n",
      "iteration 100400 / 150000: loss 36.309790\n",
      "iteration 100500 / 150000: loss 36.309464\n",
      "iteration 100600 / 150000: loss 36.309139\n",
      "iteration 100700 / 150000: loss 36.308815\n",
      "iteration 100800 / 150000: loss 36.308492\n",
      "iteration 100900 / 150000: loss 36.308170\n",
      "iteration 101000 / 150000: loss 36.307849\n",
      "iteration 101100 / 150000: loss 36.307529\n",
      "iteration 101200 / 150000: loss 36.307210\n",
      "iteration 101300 / 150000: loss 36.306891\n",
      "iteration 101400 / 150000: loss 36.306574\n",
      "iteration 101500 / 150000: loss 36.306258\n",
      "iteration 101600 / 150000: loss 36.305943\n",
      "iteration 101700 / 150000: loss 36.305628\n",
      "iteration 101800 / 150000: loss 36.305315\n",
      "iteration 101900 / 150000: loss 36.305003\n",
      "iteration 102000 / 150000: loss 36.304691\n",
      "iteration 102100 / 150000: loss 36.304381\n",
      "iteration 102200 / 150000: loss 36.304071\n",
      "iteration 102300 / 150000: loss 36.303763\n",
      "iteration 102400 / 150000: loss 36.303455\n",
      "iteration 102500 / 150000: loss 36.303148\n",
      "iteration 102600 / 150000: loss 36.302843\n",
      "iteration 102700 / 150000: loss 36.302538\n",
      "iteration 102800 / 150000: loss 36.302234\n",
      "iteration 102900 / 150000: loss 36.301931\n",
      "iteration 103000 / 150000: loss 36.301629\n",
      "iteration 103100 / 150000: loss 36.301328\n",
      "iteration 103200 / 150000: loss 36.301028\n",
      "iteration 103300 / 150000: loss 36.300728\n",
      "iteration 103400 / 150000: loss 36.300430\n",
      "iteration 103500 / 150000: loss 36.300133\n",
      "iteration 103600 / 150000: loss 36.299836\n",
      "iteration 103700 / 150000: loss 36.299540\n",
      "iteration 103800 / 150000: loss 36.299246\n",
      "iteration 103900 / 150000: loss 36.298952\n",
      "iteration 104000 / 150000: loss 36.298659\n",
      "iteration 104100 / 150000: loss 36.298367\n",
      "iteration 104200 / 150000: loss 36.298076\n",
      "iteration 104300 / 150000: loss 36.297785\n",
      "iteration 104400 / 150000: loss 36.297496\n",
      "iteration 104500 / 150000: loss 36.297208\n",
      "iteration 104600 / 150000: loss 36.296920\n",
      "iteration 104700 / 150000: loss 36.296633\n",
      "iteration 104800 / 150000: loss 36.296347\n",
      "iteration 104900 / 150000: loss 36.296062\n",
      "iteration 105000 / 150000: loss 36.295778\n",
      "iteration 105100 / 150000: loss 36.295495\n",
      "iteration 105200 / 150000: loss 36.295213\n",
      "iteration 105300 / 150000: loss 36.294931\n",
      "iteration 105400 / 150000: loss 36.294651\n",
      "iteration 105500 / 150000: loss 36.294371\n",
      "iteration 105600 / 150000: loss 36.294092\n",
      "iteration 105700 / 150000: loss 36.293814\n",
      "iteration 105800 / 150000: loss 36.293536\n",
      "iteration 105900 / 150000: loss 36.293260\n",
      "iteration 106000 / 150000: loss 36.292984\n",
      "iteration 106100 / 150000: loss 36.292710\n",
      "iteration 106200 / 150000: loss 36.292436\n",
      "iteration 106300 / 150000: loss 36.292163\n",
      "iteration 106400 / 150000: loss 36.291891\n",
      "iteration 106500 / 150000: loss 36.291619\n",
      "iteration 106600 / 150000: loss 36.291349\n",
      "iteration 106700 / 150000: loss 36.291079\n",
      "iteration 106800 / 150000: loss 36.290810\n",
      "iteration 106900 / 150000: loss 36.290542\n",
      "iteration 107000 / 150000: loss 36.290275\n",
      "iteration 107100 / 150000: loss 36.290008\n",
      "iteration 107200 / 150000: loss 36.289742\n",
      "iteration 107300 / 150000: loss 36.289478\n",
      "iteration 107400 / 150000: loss 36.289214\n",
      "iteration 107500 / 150000: loss 36.288950\n",
      "iteration 107600 / 150000: loss 36.288688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 107700 / 150000: loss 36.288426\n",
      "iteration 107800 / 150000: loss 36.288165\n",
      "iteration 107900 / 150000: loss 36.287905\n",
      "iteration 108000 / 150000: loss 36.287646\n",
      "iteration 108100 / 150000: loss 36.287388\n",
      "iteration 108200 / 150000: loss 36.287130\n",
      "iteration 108300 / 150000: loss 36.286873\n",
      "iteration 108400 / 150000: loss 36.286617\n",
      "iteration 108500 / 150000: loss 36.286362\n",
      "iteration 108600 / 150000: loss 36.286107\n",
      "iteration 108700 / 150000: loss 36.285853\n",
      "iteration 108800 / 150000: loss 36.285600\n",
      "iteration 108900 / 150000: loss 36.285348\n",
      "iteration 109000 / 150000: loss 36.285097\n",
      "iteration 109100 / 150000: loss 36.284846\n",
      "iteration 109200 / 150000: loss 36.284596\n",
      "iteration 109300 / 150000: loss 36.284347\n",
      "iteration 109400 / 150000: loss 36.284098\n",
      "iteration 109500 / 150000: loss 36.283851\n",
      "iteration 109600 / 150000: loss 36.283604\n",
      "iteration 109700 / 150000: loss 36.283358\n",
      "iteration 109800 / 150000: loss 36.283112\n",
      "iteration 109900 / 150000: loss 36.282868\n",
      "iteration 110000 / 150000: loss 36.282624\n",
      "iteration 110100 / 150000: loss 36.282380\n",
      "iteration 110200 / 150000: loss 36.282138\n",
      "iteration 110300 / 150000: loss 36.281896\n",
      "iteration 110400 / 150000: loss 36.281655\n",
      "iteration 110500 / 150000: loss 36.281415\n",
      "iteration 110600 / 150000: loss 36.281175\n",
      "iteration 110700 / 150000: loss 36.280937\n",
      "iteration 110800 / 150000: loss 36.280699\n",
      "iteration 110900 / 150000: loss 36.280461\n",
      "iteration 111000 / 150000: loss 36.280225\n",
      "iteration 111100 / 150000: loss 36.279989\n",
      "iteration 111200 / 150000: loss 36.279754\n",
      "iteration 111300 / 150000: loss 36.279519\n",
      "iteration 111400 / 150000: loss 36.279285\n",
      "iteration 111500 / 150000: loss 36.279052\n",
      "iteration 111600 / 150000: loss 36.278820\n",
      "iteration 111700 / 150000: loss 36.278588\n",
      "iteration 111800 / 150000: loss 36.278357\n",
      "iteration 111900 / 150000: loss 36.278127\n",
      "iteration 112000 / 150000: loss 36.277898\n",
      "iteration 112100 / 150000: loss 36.277669\n",
      "iteration 112200 / 150000: loss 36.277441\n",
      "iteration 112300 / 150000: loss 36.277213\n",
      "iteration 112400 / 150000: loss 36.276986\n",
      "iteration 112500 / 150000: loss 36.276760\n",
      "iteration 112600 / 150000: loss 36.276535\n",
      "iteration 112700 / 150000: loss 36.276310\n",
      "iteration 112800 / 150000: loss 36.276086\n",
      "iteration 112900 / 150000: loss 36.275863\n",
      "iteration 113000 / 150000: loss 36.275640\n",
      "iteration 113100 / 150000: loss 36.275418\n",
      "iteration 113200 / 150000: loss 36.275197\n",
      "iteration 113300 / 150000: loss 36.274976\n",
      "iteration 113400 / 150000: loss 36.274756\n",
      "iteration 113500 / 150000: loss 36.274537\n",
      "iteration 113600 / 150000: loss 36.274318\n",
      "iteration 113700 / 150000: loss 36.274100\n",
      "iteration 113800 / 150000: loss 36.273883\n",
      "iteration 113900 / 150000: loss 36.273666\n",
      "iteration 114000 / 150000: loss 36.273450\n",
      "iteration 114100 / 150000: loss 36.273235\n",
      "iteration 114200 / 150000: loss 36.273020\n",
      "iteration 114300 / 150000: loss 36.272806\n",
      "iteration 114400 / 150000: loss 36.272593\n",
      "iteration 114500 / 150000: loss 36.272380\n",
      "iteration 114600 / 150000: loss 36.272168\n",
      "iteration 114700 / 150000: loss 36.271957\n",
      "iteration 114800 / 150000: loss 36.271746\n",
      "iteration 114900 / 150000: loss 36.271536\n",
      "iteration 115000 / 150000: loss 36.271326\n",
      "iteration 115100 / 150000: loss 36.271117\n",
      "iteration 115200 / 150000: loss 36.270909\n",
      "iteration 115300 / 150000: loss 36.270701\n",
      "iteration 115400 / 150000: loss 36.270494\n",
      "iteration 115500 / 150000: loss 36.270288\n",
      "iteration 115600 / 150000: loss 36.270082\n",
      "iteration 115700 / 150000: loss 36.269877\n",
      "iteration 115800 / 150000: loss 36.269672\n",
      "iteration 115900 / 150000: loss 36.269468\n",
      "iteration 116000 / 150000: loss 36.269265\n",
      "iteration 116100 / 150000: loss 36.269062\n",
      "iteration 116200 / 150000: loss 36.268860\n",
      "iteration 116300 / 150000: loss 36.268659\n",
      "iteration 116400 / 150000: loss 36.268458\n",
      "iteration 116500 / 150000: loss 36.268258\n",
      "iteration 116600 / 150000: loss 36.268058\n",
      "iteration 116700 / 150000: loss 36.267859\n",
      "iteration 116800 / 150000: loss 36.267661\n",
      "iteration 116900 / 150000: loss 36.267463\n",
      "iteration 117000 / 150000: loss 36.267266\n",
      "iteration 117100 / 150000: loss 36.267069\n",
      "iteration 117200 / 150000: loss 36.266873\n",
      "iteration 117300 / 150000: loss 36.266678\n",
      "iteration 117400 / 150000: loss 36.266483\n",
      "iteration 117500 / 150000: loss 36.266289\n",
      "iteration 117600 / 150000: loss 36.266095\n",
      "iteration 117700 / 150000: loss 36.265902\n",
      "iteration 117800 / 150000: loss 36.265709\n",
      "iteration 117900 / 150000: loss 36.265517\n",
      "iteration 118000 / 150000: loss 36.265326\n",
      "iteration 118100 / 150000: loss 36.265135\n",
      "iteration 118200 / 150000: loss 36.264945\n",
      "iteration 118300 / 150000: loss 36.264756\n",
      "iteration 118400 / 150000: loss 36.264567\n",
      "iteration 118500 / 150000: loss 36.264378\n",
      "iteration 118600 / 150000: loss 36.264190\n",
      "iteration 118700 / 150000: loss 36.264003\n",
      "iteration 118800 / 150000: loss 36.263816\n",
      "iteration 118900 / 150000: loss 36.263630\n",
      "iteration 119000 / 150000: loss 36.263444\n",
      "iteration 119100 / 150000: loss 36.263259\n",
      "iteration 119200 / 150000: loss 36.263075\n",
      "iteration 119300 / 150000: loss 36.262891\n",
      "iteration 119400 / 150000: loss 36.262707\n",
      "iteration 119500 / 150000: loss 36.262525\n",
      "iteration 119600 / 150000: loss 36.262342\n",
      "iteration 119700 / 150000: loss 36.262161\n",
      "iteration 119800 / 150000: loss 36.261979\n",
      "iteration 119900 / 150000: loss 36.261799\n",
      "iteration 120000 / 150000: loss 36.261619\n",
      "iteration 120100 / 150000: loss 36.261439\n",
      "iteration 120200 / 150000: loss 36.261260\n",
      "iteration 120300 / 150000: loss 36.261082\n",
      "iteration 120400 / 150000: loss 36.260904\n",
      "iteration 120500 / 150000: loss 36.260726\n",
      "iteration 120600 / 150000: loss 36.260549\n",
      "iteration 120700 / 150000: loss 36.260373\n",
      "iteration 120800 / 150000: loss 36.260197\n",
      "iteration 120900 / 150000: loss 36.260022\n",
      "iteration 121000 / 150000: loss 36.259847\n",
      "iteration 121100 / 150000: loss 36.259673\n",
      "iteration 121200 / 150000: loss 36.259499\n",
      "iteration 121300 / 150000: loss 36.259326\n",
      "iteration 121400 / 150000: loss 36.259154\n",
      "iteration 121500 / 150000: loss 36.258982\n",
      "iteration 121600 / 150000: loss 36.258810\n",
      "iteration 121700 / 150000: loss 36.258639\n",
      "iteration 121800 / 150000: loss 36.258468\n",
      "iteration 121900 / 150000: loss 36.258298\n",
      "iteration 122000 / 150000: loss 36.258129\n",
      "iteration 122100 / 150000: loss 36.257960\n",
      "iteration 122200 / 150000: loss 36.257791\n",
      "iteration 122300 / 150000: loss 36.257623\n",
      "iteration 122400 / 150000: loss 36.257456\n",
      "iteration 122500 / 150000: loss 36.257289\n",
      "iteration 122600 / 150000: loss 36.257122\n",
      "iteration 122700 / 150000: loss 36.256956\n",
      "iteration 122800 / 150000: loss 36.256791\n",
      "iteration 122900 / 150000: loss 36.256626\n",
      "iteration 123000 / 150000: loss 36.256461\n",
      "iteration 123100 / 150000: loss 36.256297\n",
      "iteration 123200 / 150000: loss 36.256134\n",
      "iteration 123300 / 150000: loss 36.255971\n",
      "iteration 123400 / 150000: loss 36.255808\n",
      "iteration 123500 / 150000: loss 36.255646\n",
      "iteration 123600 / 150000: loss 36.255485\n",
      "iteration 123700 / 150000: loss 36.255324\n",
      "iteration 123800 / 150000: loss 36.255163\n",
      "iteration 123900 / 150000: loss 36.255003\n",
      "iteration 124000 / 150000: loss 36.254844\n",
      "iteration 124100 / 150000: loss 36.254685\n",
      "iteration 124200 / 150000: loss 36.254526\n",
      "iteration 124300 / 150000: loss 36.254368\n",
      "iteration 124400 / 150000: loss 36.254210\n",
      "iteration 124500 / 150000: loss 36.254053\n",
      "iteration 124600 / 150000: loss 36.253896\n",
      "iteration 124700 / 150000: loss 36.253740\n",
      "iteration 124800 / 150000: loss 36.253584\n",
      "iteration 124900 / 150000: loss 36.253429\n",
      "iteration 125000 / 150000: loss 36.253274\n",
      "iteration 125100 / 150000: loss 36.253120\n",
      "iteration 125200 / 150000: loss 36.252966\n",
      "iteration 125300 / 150000: loss 36.252812\n",
      "iteration 125400 / 150000: loss 36.252659\n",
      "iteration 125500 / 150000: loss 36.252507\n",
      "iteration 125600 / 150000: loss 36.252355\n",
      "iteration 125700 / 150000: loss 36.252203\n",
      "iteration 125800 / 150000: loss 36.252052\n",
      "iteration 125900 / 150000: loss 36.251901\n",
      "iteration 126000 / 150000: loss 36.251751\n",
      "iteration 126100 / 150000: loss 36.251601\n",
      "iteration 126200 / 150000: loss 36.251452\n",
      "iteration 126300 / 150000: loss 36.251303\n",
      "iteration 126400 / 150000: loss 36.251155\n",
      "iteration 126500 / 150000: loss 36.251007\n",
      "iteration 126600 / 150000: loss 36.250859\n",
      "iteration 126700 / 150000: loss 36.250712\n",
      "iteration 126800 / 150000: loss 36.250565\n",
      "iteration 126900 / 150000: loss 36.250419\n",
      "iteration 127000 / 150000: loss 36.250273\n",
      "iteration 127100 / 150000: loss 36.250128\n",
      "iteration 127200 / 150000: loss 36.249983\n",
      "iteration 127300 / 150000: loss 36.249839\n",
      "iteration 127400 / 150000: loss 36.249695\n",
      "iteration 127500 / 150000: loss 36.249551\n",
      "iteration 127600 / 150000: loss 36.249408\n",
      "iteration 127700 / 150000: loss 36.249265\n",
      "iteration 127800 / 150000: loss 36.249123\n",
      "iteration 127900 / 150000: loss 36.248981\n",
      "iteration 128000 / 150000: loss 36.248840\n",
      "iteration 128100 / 150000: loss 36.248699\n",
      "iteration 128200 / 150000: loss 36.248558\n",
      "iteration 128300 / 150000: loss 36.248418\n",
      "iteration 128400 / 150000: loss 36.248278\n",
      "iteration 128500 / 150000: loss 36.248139\n",
      "iteration 128600 / 150000: loss 36.248000\n",
      "iteration 128700 / 150000: loss 36.247861\n",
      "iteration 128800 / 150000: loss 36.247723\n",
      "iteration 128900 / 150000: loss 36.247586\n",
      "iteration 129000 / 150000: loss 36.247448\n",
      "iteration 129100 / 150000: loss 36.247312\n",
      "iteration 129200 / 150000: loss 36.247175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 129300 / 150000: loss 36.247039\n",
      "iteration 129400 / 150000: loss 36.246904\n",
      "iteration 129500 / 150000: loss 36.246768\n",
      "iteration 129600 / 150000: loss 36.246634\n",
      "iteration 129700 / 150000: loss 36.246499\n",
      "iteration 129800 / 150000: loss 36.246365\n",
      "iteration 129900 / 150000: loss 36.246232\n",
      "iteration 130000 / 150000: loss 36.246099\n",
      "iteration 130100 / 150000: loss 36.245966\n",
      "iteration 130200 / 150000: loss 36.245833\n",
      "iteration 130300 / 150000: loss 36.245701\n",
      "iteration 130400 / 150000: loss 36.245570\n",
      "iteration 130500 / 150000: loss 36.245439\n",
      "iteration 130600 / 150000: loss 36.245308\n",
      "iteration 130700 / 150000: loss 36.245178\n",
      "iteration 130800 / 150000: loss 36.245048\n",
      "iteration 130900 / 150000: loss 36.244918\n",
      "iteration 131000 / 150000: loss 36.244789\n",
      "iteration 131100 / 150000: loss 36.244660\n",
      "iteration 131200 / 150000: loss 36.244531\n",
      "iteration 131300 / 150000: loss 36.244403\n",
      "iteration 131400 / 150000: loss 36.244276\n",
      "iteration 131500 / 150000: loss 36.244148\n",
      "iteration 131600 / 150000: loss 36.244022\n",
      "iteration 131700 / 150000: loss 36.243895\n",
      "iteration 131800 / 150000: loss 36.243769\n",
      "iteration 131900 / 150000: loss 36.243643\n",
      "iteration 132000 / 150000: loss 36.243518\n",
      "iteration 132100 / 150000: loss 36.243393\n",
      "iteration 132200 / 150000: loss 36.243268\n",
      "iteration 132300 / 150000: loss 36.243144\n",
      "iteration 132400 / 150000: loss 36.243020\n",
      "iteration 132500 / 150000: loss 36.242896\n",
      "iteration 132600 / 150000: loss 36.242773\n",
      "iteration 132700 / 150000: loss 36.242650\n",
      "iteration 132800 / 150000: loss 36.242528\n",
      "iteration 132900 / 150000: loss 36.242406\n",
      "iteration 133000 / 150000: loss 36.242284\n",
      "iteration 133100 / 150000: loss 36.242163\n",
      "iteration 133200 / 150000: loss 36.242042\n",
      "iteration 133300 / 150000: loss 36.241922\n",
      "iteration 133400 / 150000: loss 36.241801\n",
      "iteration 133500 / 150000: loss 36.241682\n",
      "iteration 133600 / 150000: loss 36.241562\n",
      "iteration 133700 / 150000: loss 36.241443\n",
      "iteration 133800 / 150000: loss 36.241324\n",
      "iteration 133900 / 150000: loss 36.241206\n",
      "iteration 134000 / 150000: loss 36.241088\n",
      "iteration 134100 / 150000: loss 36.240970\n",
      "iteration 134200 / 150000: loss 36.240853\n",
      "iteration 134300 / 150000: loss 36.240736\n",
      "iteration 134400 / 150000: loss 36.240619\n",
      "iteration 134500 / 150000: loss 36.240503\n",
      "iteration 134600 / 150000: loss 36.240387\n",
      "iteration 134700 / 150000: loss 36.240271\n",
      "iteration 134800 / 150000: loss 36.240156\n",
      "iteration 134900 / 150000: loss 36.240041\n",
      "iteration 135000 / 150000: loss 36.239926\n",
      "iteration 135100 / 150000: loss 36.239812\n",
      "iteration 135200 / 150000: loss 36.239698\n",
      "iteration 135300 / 150000: loss 36.239585\n",
      "iteration 135400 / 150000: loss 36.239471\n",
      "iteration 135500 / 150000: loss 36.239359\n",
      "iteration 135600 / 150000: loss 36.239246\n",
      "iteration 135700 / 150000: loss 36.239134\n",
      "iteration 135800 / 150000: loss 36.239022\n",
      "iteration 135900 / 150000: loss 36.238911\n",
      "iteration 136000 / 150000: loss 36.238799\n",
      "iteration 136100 / 150000: loss 36.238689\n",
      "iteration 136200 / 150000: loss 36.238578\n",
      "iteration 136300 / 150000: loss 36.238468\n",
      "iteration 136400 / 150000: loss 36.238358\n",
      "iteration 136500 / 150000: loss 36.238249\n",
      "iteration 136600 / 150000: loss 36.238139\n",
      "iteration 136700 / 150000: loss 36.238030\n",
      "iteration 136800 / 150000: loss 36.237922\n",
      "iteration 136900 / 150000: loss 36.237814\n",
      "iteration 137000 / 150000: loss 36.237706\n",
      "iteration 137100 / 150000: loss 36.237598\n",
      "iteration 137200 / 150000: loss 36.237491\n",
      "iteration 137300 / 150000: loss 36.237384\n",
      "iteration 137400 / 150000: loss 36.237278\n",
      "iteration 137500 / 150000: loss 36.237171\n",
      "iteration 137600 / 150000: loss 36.237065\n",
      "iteration 137700 / 150000: loss 36.236960\n",
      "iteration 137800 / 150000: loss 36.236854\n",
      "iteration 137900 / 150000: loss 36.236749\n",
      "iteration 138000 / 150000: loss 36.236645\n",
      "iteration 138100 / 150000: loss 36.236540\n",
      "iteration 138200 / 150000: loss 36.236436\n",
      "iteration 138300 / 150000: loss 36.236333\n",
      "iteration 138400 / 150000: loss 36.236229\n",
      "iteration 138500 / 150000: loss 36.236126\n",
      "iteration 138600 / 150000: loss 36.236023\n",
      "iteration 138700 / 150000: loss 36.235921\n",
      "iteration 138800 / 150000: loss 36.235818\n",
      "iteration 138900 / 150000: loss 36.235717\n",
      "iteration 139000 / 150000: loss 36.235615\n",
      "iteration 139100 / 150000: loss 36.235514\n",
      "iteration 139200 / 150000: loss 36.235413\n",
      "iteration 139300 / 150000: loss 36.235312\n",
      "iteration 139400 / 150000: loss 36.235212\n",
      "iteration 139500 / 150000: loss 36.235112\n",
      "iteration 139600 / 150000: loss 36.235012\n",
      "iteration 139700 / 150000: loss 36.234912\n",
      "iteration 139800 / 150000: loss 36.234813\n",
      "iteration 139900 / 150000: loss 36.234714\n",
      "iteration 140000 / 150000: loss 36.234616\n",
      "iteration 140100 / 150000: loss 36.234517\n",
      "iteration 140200 / 150000: loss 36.234419\n",
      "iteration 140300 / 150000: loss 36.234322\n",
      "iteration 140400 / 150000: loss 36.234224\n",
      "iteration 140500 / 150000: loss 36.234127\n",
      "iteration 140600 / 150000: loss 36.234030\n",
      "iteration 140700 / 150000: loss 36.233934\n",
      "iteration 140800 / 150000: loss 36.233837\n",
      "iteration 140900 / 150000: loss 36.233741\n",
      "iteration 141000 / 150000: loss 36.233646\n",
      "iteration 141100 / 150000: loss 36.233550\n",
      "iteration 141200 / 150000: loss 36.233455\n",
      "iteration 141300 / 150000: loss 36.233361\n",
      "iteration 141400 / 150000: loss 36.233266\n",
      "iteration 141500 / 150000: loss 36.233172\n",
      "iteration 141600 / 150000: loss 36.233078\n",
      "iteration 141700 / 150000: loss 36.232984\n",
      "iteration 141800 / 150000: loss 36.232891\n",
      "iteration 141900 / 150000: loss 36.232798\n",
      "iteration 142000 / 150000: loss 36.232705\n",
      "iteration 142100 / 150000: loss 36.232612\n",
      "iteration 142200 / 150000: loss 36.232520\n",
      "iteration 142300 / 150000: loss 36.232428\n",
      "iteration 142400 / 150000: loss 36.232336\n",
      "iteration 142500 / 150000: loss 36.232245\n",
      "iteration 142600 / 150000: loss 36.232153\n",
      "iteration 142700 / 150000: loss 36.232063\n",
      "iteration 142800 / 150000: loss 36.231972\n",
      "iteration 142900 / 150000: loss 36.231882\n",
      "iteration 143000 / 150000: loss 36.231791\n",
      "iteration 143100 / 150000: loss 36.231702\n",
      "iteration 143200 / 150000: loss 36.231612\n",
      "iteration 143300 / 150000: loss 36.231523\n",
      "iteration 143400 / 150000: loss 36.231434\n",
      "iteration 143500 / 150000: loss 36.231345\n",
      "iteration 143600 / 150000: loss 36.231256\n",
      "iteration 143700 / 150000: loss 36.231168\n",
      "iteration 143800 / 150000: loss 36.231080\n",
      "iteration 143900 / 150000: loss 36.230993\n",
      "iteration 144000 / 150000: loss 36.230905\n",
      "iteration 144100 / 150000: loss 36.230818\n",
      "iteration 144200 / 150000: loss 36.230731\n",
      "iteration 144300 / 150000: loss 36.230644\n",
      "iteration 144400 / 150000: loss 36.230558\n",
      "iteration 144500 / 150000: loss 36.230472\n",
      "iteration 144600 / 150000: loss 36.230386\n",
      "iteration 144700 / 150000: loss 36.230300\n",
      "iteration 144800 / 150000: loss 36.230215\n",
      "iteration 144900 / 150000: loss 36.230130\n",
      "iteration 145000 / 150000: loss 36.230045\n",
      "iteration 145100 / 150000: loss 36.229960\n",
      "iteration 145200 / 150000: loss 36.229876\n",
      "iteration 145300 / 150000: loss 36.229792\n",
      "iteration 145400 / 150000: loss 36.229708\n",
      "iteration 145500 / 150000: loss 36.229624\n",
      "iteration 145600 / 150000: loss 36.229541\n",
      "iteration 145700 / 150000: loss 36.229458\n",
      "iteration 145800 / 150000: loss 36.229375\n",
      "iteration 145900 / 150000: loss 36.229293\n",
      "iteration 146000 / 150000: loss 36.229210\n",
      "iteration 146100 / 150000: loss 36.229128\n",
      "iteration 146200 / 150000: loss 36.229046\n",
      "iteration 146300 / 150000: loss 36.228965\n",
      "iteration 146400 / 150000: loss 36.228883\n",
      "iteration 146500 / 150000: loss 36.228802\n",
      "iteration 146600 / 150000: loss 36.228721\n",
      "iteration 146700 / 150000: loss 36.228641\n",
      "iteration 146800 / 150000: loss 36.228560\n",
      "iteration 146900 / 150000: loss 36.228480\n",
      "iteration 147000 / 150000: loss 36.228400\n",
      "iteration 147100 / 150000: loss 36.228320\n",
      "iteration 147200 / 150000: loss 36.228241\n",
      "iteration 147300 / 150000: loss 36.228162\n",
      "iteration 147400 / 150000: loss 36.228083\n",
      "iteration 147500 / 150000: loss 36.228004\n",
      "iteration 147600 / 150000: loss 36.227926\n",
      "iteration 147700 / 150000: loss 36.227847\n",
      "iteration 147800 / 150000: loss 36.227769\n",
      "iteration 147900 / 150000: loss 36.227691\n",
      "iteration 148000 / 150000: loss 36.227614\n",
      "iteration 148100 / 150000: loss 36.227537\n",
      "iteration 148200 / 150000: loss 36.227459\n",
      "iteration 148300 / 150000: loss 36.227383\n",
      "iteration 148400 / 150000: loss 36.227306\n",
      "iteration 148500 / 150000: loss 36.227230\n",
      "iteration 148600 / 150000: loss 36.227153\n",
      "iteration 148700 / 150000: loss 36.227077\n",
      "iteration 148800 / 150000: loss 36.227002\n",
      "iteration 148900 / 150000: loss 36.226926\n",
      "iteration 149000 / 150000: loss 36.226851\n",
      "iteration 149100 / 150000: loss 36.226776\n",
      "iteration 149200 / 150000: loss 36.226701\n",
      "iteration 149300 / 150000: loss 36.226626\n",
      "iteration 149400 / 150000: loss 36.226552\n",
      "iteration 149500 / 150000: loss 36.226478\n",
      "iteration 149600 / 150000: loss 36.226404\n",
      "iteration 149700 / 150000: loss 36.226330\n",
      "iteration 149800 / 150000: loss 36.226257\n",
      "iteration 149900 / 150000: loss 36.226183\n",
      "That took 7.359533s\n"
     ]
    }
   ],
   "source": [
    "# Ở trong tệp linear_regression.py, cài đặt hàm LinearRegression.train() và chạy\n",
    "# hàm đó với code sau\n",
    "from models.linear_regression import LinearRegression\n",
    "clf = LinearRegression()\n",
    "tic = time.time()\n",
    "loss_hist = clf.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
    "                      num_iters=150000, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22f1c853978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Một chiến thuật debug hiệu quả được sử dụng đó là vẽ ra lịch sử mất mát (loss \n",
    "# history) như là một hàm với số lần lặp.\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.000000\n",
      "validation accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Cài đặt hàm LinearRegression.predict đánh giá hiệu năng mô hình trên cả tập\n",
    "# huấn luyện và tệp kiểm tra.\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print('validation accuracy: %f' % (np.mean(y_test == y_test_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "D:\\ML\\UET ML INT3405 1\\week 4\\models\\linear_loss.py:78: RuntimeWarning: overflow encountered in square\n",
      "  loss = (np.sum(diff ** 2) + reg * np.sum(W ** 2)) / (2 * N)\n",
      "D:\\ML\\UET ML INT3405 1\\week 4\\models\\linear_loss.py:78: RuntimeWarning: overflow encountered in double_scalars\n",
      "  loss = (np.sum(diff ** 2) + reg * np.sum(W ** 2)) / (2 * N)\n",
      "D:\\ML\\UET ML INT3405 1\\week 4\\models\\linear_regression.py:43: RuntimeWarning: invalid value encountered in subtract\n",
      "  self.W -= learning_rate * grad\n",
      "D:\\ML\\UET ML INT3405 1\\week 4\\models\\linear_loss.py:79: RuntimeWarning: overflow encountered in add\n",
      "  dW = (np.dot(np.transpose(X), diff) + reg * W) / N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.5691242345789\n"
     ]
    }
   ],
   "source": [
    "# Sử dụng tập kiểm tra để điều chỉnh các siêu tham số (độ lớn của reg và tỉ\n",
    "# lệ học. Bạn nên thực nghiệm với nhiều khoảng giá trị của 2 siêu tham số này\n",
    "# Nếu bạn đủ cẩn thận, bạn có thể đạt độ chính xác ... trên tập kiểm tra.\n",
    "learning_rates = [1e-7, 5e-5]\n",
    "regularization_strengths = [5e4, 1e5]\n",
    "\n",
    "import sys\n",
    "\n",
    "# kết quả là một từ điển ánh xạ từ tuple có dạng (reg, lr) sang tuple có dạng\n",
    "# (train_acc, test_acc). Độ chính xác chỉ đơn giản là tỉ lệ mẫu dự đoán chính\n",
    "# xác trên toàn tập dữ liệu.\n",
    "results = {}\n",
    "best_loss = sys.float_info.max   # Hiệu năng tốt nhất mà chúng ta sẽ đạt được.\n",
    "best_linear = None # Mô hình LinearRegression có hiệu năng tốt nhất.\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Viết code chọn các siêu tham số tốt nhất bằng cách điều chỉnh trên tập kiểm  #\n",
    "# tra. Với mỗi tổ hợp siêu tham số, huấn luyện một mô hình LinearRegression    #\n",
    "# trên tập huấn luyện, tính toán độ chính xác trên tập huấn luyện và tập kiểm  #\n",
    "# tra, và lưu những con số này vào từ điển kết quả. Thêm vào đó, lưu hiệu năng #\n",
    "# tốt nhất trên tập kiểm tra vào best_val và mô hình LinearRegression tương    #\n",
    "# ứng vào best_svm.                                                            #  \n",
    "#                                                                              #\n",
    "# Gợi ý: Bạn nên sử dụng số vòng lặp (num_iters) nhỏ khi xây dựng code kiểm    #\n",
    "# tra để mô hình không mất quá nhiều thời gian để huấn luyện. Khi đã chắc chắn,#\n",
    "# bạn nên trả về kết quả với số vòng lặp lớn                                   #\n",
    "################################################################################\n",
    "\n",
    "best_lr = 0\n",
    "best_reg = 0\n",
    "\n",
    "for reg in np.linspace(5e4, 1e5, 100):\n",
    "    for lr in np.linspace(1e-7, 5e-5, 100):\n",
    "        model = LinearRegression()\n",
    "        model.train(X_train, y_train, lr, reg, num_iters=2000)\n",
    "        \n",
    "        loss, grad = model.loss(X_test, y_test, reg)\n",
    "        if best_loss > loss:\n",
    "            best_loss = loss\n",
    "            best_lr = lr\n",
    "            best_reg = reg\n",
    "            \n",
    "model = LinearRegression()\n",
    "model.train(X_train, y_train, best_lr, best_reg, num_iters=20000)\n",
    "loss, grad = model.loss(X_test, y_test, best_reg)\n",
    "print(loss)\n",
    "\n",
    "pass\n",
    "################################################################################\n",
    "#                              KẾT THÚC                                        #\n",
    "################################################################################\n",
    "    \n",
    "# In kết quả\n",
    "    # for lr, reg in sorted(results):\n",
    "    #     train_accuracy, test_accuracy = results[(lr, reg)]\n",
    "    #     print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "    #                 lr, reg, train_accuracy, test_accuracy))\n",
    "\n",
    "    # print('best validation accuracy achieved during cross-validation: %f' % best_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26e6ddce780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize kết quả kiểm thử chéo\n",
    "import math\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('Boston training accuracy')\n",
    "\n",
    "# vẽ hiệu năng trên tập kiểm tra\n",
    "colors = [results[x][1] for x in results] # kích thước mặc định của marker là 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('Boston test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
